<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>blog.thesparktree.com</title>
   
   <link>https://blog.thesparktree.com</link>
   <description>Devops posts & guides about interesting tech like Docker, Letsencrypt, Chef, Angular, Automation, API's or other topics that you should know about. </description>
   <language>en-uk</language>
   <managingEditor> Jason Kulatunga</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Building Multi-Arch Docker Images via Github Actions</title>
	  <link>/docker-multi-arch-github-actions</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2022-06-11T04:19:33-05:00</pubDate>
	  <guid>/docker-multi-arch-github-actions</guid>
	  <description><![CDATA[
	     <p>I recently found myself needing to generate a multi-arch Docker image for one of my projects - specifically an ARM64 compatible image.
While its well known that Docker’s <code class="language-plaintext highlighter-rouge">buildx</code> tooling supports multi-arch builds, it can be complicated getting it working correctly
via Github Actions.</p>

<h2 id="what-is-a-multi-arch-docker-image">What is a Multi-Arch Docker Image?</h2>

<p>Before we go any further, we should discuss how Docker Images (&amp; Multi-Arch Docker Images) actually work.</p>

<blockquote>
  <p>Each Docker image is represented by a manifest. A manifest is a JSON file containing all the information about a Docker 
image. This includes references to each of its layers, their corresponding sizes, the hash of the image, its size and 
also the platform it’s supposed to work on. This manifest can then be referenced by a tag so that it’s easy to find.</p>
</blockquote>

<p>A multi-arch image is actually just a manifest that contains multiple entries, 1 for each platform.</p>

<p><img src="https://blog.thesparktree.com/assets/images/docker-multi-arch-manifest.png" alt="docker multi-arch manifest" style="max-height: 500px;" /></p>

<p>To learn more, see this <a href="https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/">Docker blog post</a></p>

<h2 id="basic-docker-build-via-github-actions">Basic Docker Build via Github Actions</h2>

<p>Now that we know what a multi-arch docker image looks like under the hood, lets get started with a simple Github Action
to build a Docker image.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">name</span><span class="pi">:</span> <span class="s">Docker</span>
<span class="na">on</span><span class="pi">:</span>
  <span class="na">push</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">main'</span><span class="pi">]</span>
<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">docker</span><span class="pi">:</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>
    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Checkout repository</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v3</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Login to DockerHub</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/login-action@v2</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">username</span><span class="pi">:</span> <span class="s">${{ secrets.DOCKERHUB_USERNAME }}</span>
          <span class="na">password</span><span class="pi">:</span> <span class="s">${{ secrets.DOCKERHUB_TOKEN }}</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build and push</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/build-push-action@v3</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">push</span><span class="pi">:</span> <span class="kc">true</span>
          <span class="na">tags</span><span class="pi">:</span> <span class="s">user/app:latest</span>
</code></pre></div></div>

<h2 id="migrate-to-buildx">Migrate to Buildx</h2>

<p>The first thing we need to do is add the <code class="language-plaintext highlighter-rouge">setup-buildx-action</code> step.</p>

<blockquote>
  <p>Docker Buildx is a CLI plugin that extends the docker command with the full support 
of the features provided by Moby BuildKit builder toolkit. It provides the same 
user experience as docker build with many new features like creating scoped 
builder instances and building against multiple nodes concurrently.</p>
</blockquote>

<p>Unfortunately Buildx is not enabled by default, so even though <code class="language-plaintext highlighter-rouge">docker</code> is available in our Github Action VM, we’ll need to enable <code class="language-plaintext highlighter-rouge">buildx</code> mode.</p>

<div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">--- workflow.yaml       2022-06-12 08:09:34.000000000 -0700
</span><span class="gi">+++ workflow-updated.yaml       2022-06-12 08:10:12.000000000 -0700
</span><span class="p">@@ -1,20 +1,22 @@</span>
 name: Docker
 on:
   push:
     branches: ['main']
 jobs:
   docker:
     runs-on: ubuntu-latest
     steps:
       - name: Checkout repository
         uses: actions/checkout@v3
<span class="gi">+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
</span>       - name: Login to DockerHub
         uses: docker/login-action@v2
         with:
           username: ${{ secrets.DOCKERHUB_USERNAME }}
           password: ${{ secrets.DOCKERHUB_TOKEN }}
       - name: Build and push
         uses: docker/build-push-action@v3
         with:
           push: true
           tags: user/app:latest
\ No newline at end of file
</code></pre></div></div>

<h2 id="qemu-support">QEMU Support</h2>
<p>After enabling <code class="language-plaintext highlighter-rouge">buildx</code>, the next change we need to make is to enable <code class="language-plaintext highlighter-rouge">QEMU</code>.</p>

<blockquote>
  <p>QEMU is a free and open-source emulator. It can interoperate with Kernel-based 
Virtual Machine (KVM) to run virtual machines at near-native speed. QEMU can also 
do emulation for user-level processes, allowing applications compiled for one 
architecture to run on another</p>
</blockquote>

<p>GitHub Actions only provides a small set of host system types: <a href="https://github.com/actions/virtual-environments"><code class="language-plaintext highlighter-rouge">windows</code>, <code class="language-plaintext highlighter-rouge">macos</code> &amp; <code class="language-plaintext highlighter-rouge">ubuntu</code></a> – all running on <code class="language-plaintext highlighter-rouge">x86_64</code> architecture. 
If you need to compile binaries/Docker images for other OS’s or architectures, you can use the <code class="language-plaintext highlighter-rouge">QEMU</code> Github Action.</p>

<div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">--- workflow.yaml       2022-06-12 08:32:32.000000000 -0700
</span><span class="gi">+++ workflow-updated.yaml       2022-06-12 08:32:56.000000000 -0700
</span><span class="p">@@ -1,22 +1,26 @@</span>
 name: Docker
 on:
   push:
     branches: ['main']
 jobs:
   docker:
     runs-on: ubuntu-latest
     steps:
       - name: Checkout repository
         uses: actions/checkout@v3
<span class="gi">+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v2
+        with:
+          platforms: 'arm64,arm'
</span>       - name: Set up Docker Buildx
         uses: docker/setup-buildx-action@v2
       - name: Login to DockerHub
         uses: docker/login-action@v2
         with:
           username: ${{ secrets.DOCKERHUB_USERNAME }}
           password: ${{ secrets.DOCKERHUB_TOKEN }}
       - name: Build and push
         uses: docker/build-push-action@v3
         with:
           push: true
           tags: user/app:latest
\ No newline at end of file
</code></pre></div></div>

<blockquote>
  <p>NOTE: you must add the <code class="language-plaintext highlighter-rouge">QEMU</code> step before the <code class="language-plaintext highlighter-rouge">buildx</code> step. 
By default <code class="language-plaintext highlighter-rouge">QEMU</code> will create almost a dozen vm’s. You’ll want to limit it to just the architectures you care about.</p>
</blockquote>

<h2 id="architecture-specific-dockerfile-instructions">Architecture Specific Dockerfile Instructions</h2>

<p>Depending on the content of your Dockerfile, at this point you may be done. 
The <code class="language-plaintext highlighter-rouge">setup-qemu-action</code> will create 2 (or more) VMs, and the <code class="language-plaintext highlighter-rouge">build-push-action</code> will 
compile your Dockerfile for various architectures, and push them to <code class="language-plaintext highlighter-rouge">Docker Hub</code> (within the same manifest).</p>

<p>However, if you need to conditionalize your Dockerfile instructions depending on which architecture you’re building,
you’ll need to make some additional changes.</p>

<p>Under the hood, the <code class="language-plaintext highlighter-rouge">build-push-action</code> will provide the <code class="language-plaintext highlighter-rouge">--platform</code> flag to <code class="language-plaintext highlighter-rouge">docker buildx</code>. 
This will <a href="https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope">automatically set</a> the following build <code class="language-plaintext highlighter-rouge">ARG</code>s:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">TARGETPLATFORM</code> - platform of the build result. Eg <code class="language-plaintext highlighter-rouge">linux/amd64</code>, <code class="language-plaintext highlighter-rouge">linux/arm/v7</code>, <code class="language-plaintext highlighter-rouge">windows/amd64</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">TARGETOS</code> - OS component of <code class="language-plaintext highlighter-rouge">TARGETPLATFORM</code></li>
  <li><code class="language-plaintext highlighter-rouge">TARGETARCH</code> - architecture component of <code class="language-plaintext highlighter-rouge">TARGETPLATFORM</code></li>
  <li><code class="language-plaintext highlighter-rouge">TARGETVARIANT</code> - variant component of <code class="language-plaintext highlighter-rouge">TARGETPLATFORM</code></li>
  <li><code class="language-plaintext highlighter-rouge">BUILDPLATFORM</code> - platform of the node performing the build.</li>
  <li><code class="language-plaintext highlighter-rouge">BUILDOS</code> - OS component of <code class="language-plaintext highlighter-rouge">BUILDPLATFORM</code></li>
  <li><code class="language-plaintext highlighter-rouge">BUILDARCH</code> - architecture component of <code class="language-plaintext highlighter-rouge">BUILDPLATFORM</code></li>
  <li><code class="language-plaintext highlighter-rouge">BUILDVARIANT</code> - variant component of <code class="language-plaintext highlighter-rouge">BUILDPLATFORM</code></li>
</ul>

<p>To use these variables to conditionally download arch specific dependencies, you can modify your Dockerfile like so:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> debian:bullseye-slim</span>
<span class="k">ARG</span><span class="s"> TARGETARCH</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> curl <span class="se">\
</span>    <span class="o">&amp;&amp;</span>  <span class="k">case</span> <span class="k">${</span><span class="nv">TARGETARCH</span><span class="k">}</span> <span class="k">in</span> <span class="se">\
</span>            <span class="s2">"amd64"</span><span class="p">)</span>  <span class="nv">S6_ARCH</span><span class="o">=</span>amd64  <span class="p">;;</span> <span class="se">\
</span>            <span class="s2">"arm64"</span><span class="p">)</span>  <span class="nv">S6_ARCH</span><span class="o">=</span>aarch64  <span class="p">;;</span> <span class="se">\
</span>        <span class="k">esac</span> <span class="se">\
</span>    <span class="o">&amp;&amp;</span> curl https://github.com/just-containers/s6-overlay/releases/download/v1.21.8.0/s6-overlay-<span class="k">${</span><span class="nv">S6_ARCH</span><span class="k">}</span>.tar.gz <span class="nt">-L</span> <span class="nt">-s</span> <span class="nt">--output</span> /tmp/s6-overlay-<span class="k">${</span><span class="nv">S6_ARCH</span><span class="k">}</span>.tar.gz <span class="se">\
</span>    <span class="o">&amp;&amp;</span> curl <span class="nt">-L</span> https://dl.influxdata.com/influxdb/releases/influxdb2-2.2.0-<span class="k">${</span><span class="nv">TARGETARCH</span><span class="k">}</span>.deb <span class="nt">--output</span> /tmp/influxdb2-2.2.0-<span class="k">${</span><span class="nv">TARGETARCH</span><span class="k">}</span>.deb <span class="se">\
</span>    <span class="o">&amp;&amp;</span> ....
</code></pre></div></div>

<h2 id="troubleshooting">Troubleshooting</h2>

<h3 id="q-i-enabled-multi-arch-builds-and-my-builds-take-1h-what-gives">Q: I enabled Multi-arch builds and my builds take 1h+, what gives?</h3>
<p><strong>A:</strong> This seems to be a <a href="https://github.com/docker/setup-qemu-action/issues/22">known issue with <code class="language-plaintext highlighter-rouge">QEMU</code></a>.
I’ve also run into this with NPM installs and TypeScript compilation. 
My workaround was to move non-architecture specific compilation before the Docker build &amp; QEMU steps.
This means the steps are running outside the VMs and my build time dropped down to ~15 minutes, which is much more reasonable.</p>

<h1 id="references">References</h1>
<ul>
  <li>https://docs.docker.com/desktop/multi-arch/</li>
  <li>https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/</li>
  <li>https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope</li>
  <li>https://www.docker.com/blog/faster-multi-platform-builds-dockerfile-cross-compilation-guide/</li>
  <li>https://github.com/BretFisher/multi-platform-docker-build</li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Running Cron in Docker</title>
	  <link>/cron-in-docker</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2021-04-26T04:19:33-05:00</pubDate>
	  <guid>/cron-in-docker</guid>
	  <description><![CDATA[
	     <p>Running <code class="language-plaintext highlighter-rouge">cron</code> in a Docker container is incredibly difficult to do correctly.
This is partially because <code class="language-plaintext highlighter-rouge">cron</code> was designed to run in an environment that looks very different than a docker container,
and partially because what we traditionally think of as <code class="language-plaintext highlighter-rouge">cron</code> is actually a different tool in each flavor of Linux.</p>

<p>As always, here’s a Github repo with working code if you want to skip ahead:</p>

<div class="github-widget" data-repo="AnalogJ/docker-cron"></div>

<h2 id="what-is-cron">What is <code class="language-plaintext highlighter-rouge">cron</code></h2>

<blockquote>
  <p>The software utility <strong>cron</strong> also known as <strong>cron job</strong> is a time-based job scheduler in Unix-like computer operating
systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts) to run
periodically at fixed times, dates, or intervals. It typically automates system maintenance or administration—though its
general-purpose nature makes it useful for things like downloading files from the Internet and downloading email at regular
intervals.</p>
</blockquote>

<p><a href="https://en.wikipedia.org/wiki/Cron">https://en.wikipedia.org/wiki/Cron</a></p>

<p>Basically it’s a language/platform/distro agnostic tool for scheduling tasks/scripts to run automatically at some interval.</p>

<h2 id="differences-between-various-versions">Differences between various versions</h2>

<p>Though <code class="language-plaintext highlighter-rouge">cron</code>’s API is standardized, there are multiple implementations, which vary as the default for various distros
(<a href="http://www.jimpryor.net/linux/dcron.html">dcron</a>, <a href="https://github.com/cronie-crond/cronie">cronie</a>,
<a href="http://fcron.free.fr/">fcron</a> and <a href="https://directory.fsf.org/wiki/Vixie-cron">vixie-cron</a>)</p>

<p>To add to the complexity, some of <code class="language-plaintext highlighter-rouge">cron</code>’s functionality is actually defined/provided by <code class="language-plaintext highlighter-rouge">anachron</code>. <code class="language-plaintext highlighter-rouge">anacron</code> was
previously a stand-alone binary which was used to run commands periodically with a frequency defined in days. It works
a little different from cron; assumes that a machine will not be powered on all the time.</p>

<p>So to summarize, there are multiple <code class="language-plaintext highlighter-rouge">cron</code> implementations, with differing flags &amp; features, some with <code class="language-plaintext highlighter-rouge">anacron</code>
functionality built-in, and some without. In the following sections I’ll call out different solutions for different
distros/<code class="language-plaintext highlighter-rouge">cron</code> implementations (keep an eye out for <code class="language-plaintext highlighter-rouge">NOTE:</code> blocks)</p>

<blockquote>
  <p>NOTE: Installation instructions differ per distro</p>

  <ul>
    <li>Debian/Ubuntu: <code class="language-plaintext highlighter-rouge">apt-get update &amp;&amp; apt-get install -y cron &amp;&amp; cron</code></li>
    <li>Alpine: <code class="language-plaintext highlighter-rouge">which crond</code> # comes pre-installed</li>
    <li>Centos: <code class="language-plaintext highlighter-rouge">yum install -y cronie &amp;&amp; crond -V</code></li>
  </ul>
</blockquote>

<h2 id="config-file">Config File</h2>

<p>Let’s start with a simple issue. <code class="language-plaintext highlighter-rouge">cron</code> is designed to run in a multi-user environment, which is great when you’re running
<code class="language-plaintext highlighter-rouge">cron</code> on a desktop, but less useful when running <code class="language-plaintext highlighter-rouge">cron</code> in a docker container.</p>

<p>Rather than creating a user specific <code class="language-plaintext highlighter-rouge">crontab</code> file, in our Docker container we’ll modify the system-level <code class="language-plaintext highlighter-rouge">crontab</code>.</p>

<p>Let’s create/update a file called <code class="language-plaintext highlighter-rouge">/etc/crontab</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name command to be executed

* * * * * root date
</code></pre></div></div>

<p>This file will configure <code class="language-plaintext highlighter-rouge">cron</code> to run the <code class="language-plaintext highlighter-rouge">date</code> command every minute. We’ll talk about the output for this command in a later section.</p>

<blockquote>
  <p>NOTE:</p>

  <ul>
    <li>Debian/Ubuntu: replace the existing <code class="language-plaintext highlighter-rouge">/etc/crontab</code> which contains <code class="language-plaintext highlighter-rouge">anacron</code> entries</li>
    <li>Alpine: the crontab file should be written to <code class="language-plaintext highlighter-rouge">/var/spool/cron/crontabs/root</code>, also the format is slightly different (the <code class="language-plaintext highlighter-rouge">user</code> field should be removed).</li>
    <li>Centos: replace the existing <code class="language-plaintext highlighter-rouge">/etc/crontab</code> which contains <code class="language-plaintext highlighter-rouge">anacron</code> entries</li>
  </ul>
</blockquote>

<h2 id="foreground">Foreground</h2>

<p>Now that we have created a <code class="language-plaintext highlighter-rouge">cron</code> config file, we need to start <code class="language-plaintext highlighter-rouge">cron</code>. On a normal system, we would start <code class="language-plaintext highlighter-rouge">cron</code> as a
daemon, a background process usually managed by service manager. In the Docker world, the convention is 1 process per container,
 running in the foreground.</p>

<p>Thankfully most <code class="language-plaintext highlighter-rouge">cron</code> implementations support this, even though the flags may be different.</p>

<blockquote>
  <p>NOTE: Running cron in the foreground differs per distro</p>

  <ul>
    <li>Debian/Ubuntu: <code class="language-plaintext highlighter-rouge">cron -f -l 2</code></li>
    <li>Alpine: <code class="language-plaintext highlighter-rouge">crond -f -l 2</code></li>
    <li>Centos: <code class="language-plaintext highlighter-rouge">crond -n</code></li>
  </ul>
</blockquote>

<h2 id="environment">Environment</h2>

<p>As mentioned earlier, <code class="language-plaintext highlighter-rouge">cron</code> is designed to work in a multi-user environment, which also means the <code class="language-plaintext highlighter-rouge">cron</code> daemon cannot
make assumptions about the runtime environment (process environmental variables, etc). The way <code class="language-plaintext highlighter-rouge">cron</code> enforces this is
by starting each job with a custom environment, using an implementation specific environmental variables file (usually <code class="language-plaintext highlighter-rouge">/etc/environment</code>)</p>

<p>Since using environmental variables is a common configuration mechanism for Docker containers, we need a way to ensure the current
Docker container environment is passed into the cron sub-processes. The best way to do this is by creating a custom
entrypoint script which dumps the environment to the <code class="language-plaintext highlighter-rouge">cron</code> environment file, before starting <code class="language-plaintext highlighter-rouge">cron</code> in the foreground.</p>

<p>Create the following <code class="language-plaintext highlighter-rouge">/entrypoint.sh</code> script in your Docker image.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>

<span class="nb">env</span> <span class="o">&gt;&gt;</span> /etc/environment

<span class="c"># start cron in the foreground (replacing the current process)</span>
<span class="nb">exec</span> <span class="s2">"cron -f"</span>
</code></pre></div></div>

<blockquote>
  <p>NOTE:</p>

  <ul>
    <li>Centos: unfortunately <code class="language-plaintext highlighter-rouge">cronie</code> doesn’t read variables from <code class="language-plaintext highlighter-rouge">/etc/environment</code>.
      <ul>
        <li>You’ll need to manually source it before your script: <code class="language-plaintext highlighter-rouge">* * * * * root . /etc/environment; date</code></li>
        <li>
          <p>If you have multiple entries in your <code class="language-plaintext highlighter-rouge">crontab</code>, you can change the default <code class="language-plaintext highlighter-rouge">SHELL</code> for your <code class="language-plaintext highlighter-rouge">crontab</code> file, and make use of <code class="language-plaintext highlighter-rouge">BASH_ENV</code></p>

          <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   SHELL=/bin/bash
   BASH_ENV=/etc/environment
   * * * * * root echo "${CUSTOM_ENV_VAR}"
</code></pre></div>          </div>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="stdoutstderr">STDOUT/STDERR</h2>

<p>If you’ve been following along so far, you might be wondering why you’re not seeing any output from <code class="language-plaintext highlighter-rouge">date</code> in your
terminal. That’s because even though <code class="language-plaintext highlighter-rouge">cron</code> is running in the foreground, the output from its child processes is designed
to go to a log file (traditionally at <code class="language-plaintext highlighter-rouge">/var/log/cron</code>). Again, this might be fine on a standard linux host, but it’s
sub-optimal for a Docker container.</p>

<p>Let’s use some shell redirect magic to redirect the <code class="language-plaintext highlighter-rouge">STDOUT</code> and <code class="language-plaintext highlighter-rouge">STDERR</code> from our <code class="language-plaintext highlighter-rouge">cron</code> jobs, to the <code class="language-plaintext highlighter-rouge">cron</code> process
(running as the primary process in the Docker container, with <a href="https://en.wikipedia.org/wiki/Process_identifier">PID 1</a>).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># &gt;/proc/1/fd/1 redirects STDOUT from the `date` command to PID1's STDOUT
# 2&gt;/proc/1/fd/2 redirects STDERR from the `date` command to PID1's STDERR

* * * * * root date &gt;/proc/1/fd/1 2&gt;/proc/1/fd/2
</code></pre></div></div>

<p>While <code class="language-plaintext highlighter-rouge">&gt;/proc/1/fd/1 2&gt;/proc/1/fd/2</code> may look intimidating, it’s the most consistent way to pass <code class="language-plaintext highlighter-rouge">cronjob</code> logs to the container’s
STDOUT, without leveraging clunky solutions like <code class="language-plaintext highlighter-rouge">crond &amp;&amp; tail -f /var/log/cron</code></p>

<blockquote>
  <p>NOTE: this is unnecessary in Alpine, as long as you start cron with the following command:</p>
  <ul>
    <li>Alpine: <code class="language-plaintext highlighter-rouge">crond -f -l 2</code></li>
  </ul>
</blockquote>

<h2 id="cron-package-installation">Cron package installation</h2>

<p>Now that we have a working container with <code class="language-plaintext highlighter-rouge">cron</code>, we should take the time to clean up some of the unused cruft that our
<code class="language-plaintext highlighter-rouge">cron</code> package installs, specifically configs for <code class="language-plaintext highlighter-rouge">anacron</code>.</p>

<blockquote>
  <p>NOTE:</p>

  <ul>
    <li>Debian/Ubuntu: <code class="language-plaintext highlighter-rouge">rm -rf /etc/cron.*/*</code></li>
    <li>Alpine: <code class="language-plaintext highlighter-rouge">rm -rf /etc/periodic</code></li>
    <li>Centos: <code class="language-plaintext highlighter-rouge">rm -rf /etc/cron.*/*</code></li>
  </ul>
</blockquote>

<h2 id="kill">Kill</h2>

<p>Finally, as you’ve been playing around, you may have noticed that it’s difficult to kill the container running <code class="language-plaintext highlighter-rouge">cron</code>.
You may have had to use <code class="language-plaintext highlighter-rouge">docker kill</code> or <code class="language-plaintext highlighter-rouge">docker-compose kill</code> to terminate the container, rather than using <code class="language-plaintext highlighter-rouge">ctrl + C</code> or <code class="language-plaintext highlighter-rouge">docker stop</code>.</p>

<p>Unfortunately, it seems like <code class="language-plaintext highlighter-rouge">SIGINT</code> is not always correctly handled by <code class="language-plaintext highlighter-rouge">cron</code> implementations when running in the foreground.</p>

<p>After researching a couple of alternatives, the only solution that seemed to work was using a process supervisor (like
<code class="language-plaintext highlighter-rouge">tini</code> or <code class="language-plaintext highlighter-rouge">s6-overlay</code>). Since <code class="language-plaintext highlighter-rouge">tini</code> was merged into Docker 1.13, technically, you can use it transparently by passing
<code class="language-plaintext highlighter-rouge">--init</code> to your docker run command. In practice you often can’t because your cluster manager doesn’t support it.</p>

<blockquote>
  <p>NOTE: this is unnecessary in Centos, SIGTERM works correctly with <code class="language-plaintext highlighter-rouge">cronie</code> in the foreground.</p>
</blockquote>

<h2 id="putting-it-all-together">Putting it all together</h2>

<p>Let’s see what all of this would look like for an <code class="language-plaintext highlighter-rouge">ubuntu</code> base image.</p>

<p>Create a <code class="language-plaintext highlighter-rouge">Dockerfile</code></p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> cron <span class="o">&amp;&amp;</span> which cron <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">rm</span> <span class="nt">-rf</span> /etc/cron.<span class="k">*</span>/<span class="k">*</span>

<span class="k">COPY</span><span class="s"> entrypoint.sh /entrypoint.sh</span>

<span class="k">ENTRYPOINT</span><span class="s"> ["/entrypoint.sh"]</span>
<span class="k">CMD</span><span class="s"> ["cron","-f", "-l", "2"]</span>
</code></pre></div></div>

<p>Create an <code class="language-plaintext highlighter-rouge">entrypoint.sh</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>

<span class="nb">env</span> <span class="o">&gt;&gt;</span> /etc/environment

<span class="c"># execute CMD</span>
<span class="nb">echo</span> <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>
<span class="nb">exec</span> <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span>

</code></pre></div></div>

<p>Create a <code class="language-plaintext highlighter-rouge">crontab</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name command to be executed

* * * * * root date &gt;/proc/1/fd/1 2&gt;/proc/1/fd/2
* * * * * root echo "${CUSTOM_ENV_VAR}" &gt;/proc/1/fd/1 2&gt;/proc/1/fd/2

# An empty line is required at the end of this file for a valid cron file.

</code></pre></div></div>

<p>Build the Dockerfile and run it with <code class="language-plaintext highlighter-rouge">--init</code> (package <code class="language-plaintext highlighter-rouge">tini</code> or <code class="language-plaintext highlighter-rouge">s6-overlay</code> for containers in production)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> analogj/cron <span class="nb">.</span>
docker run <span class="nt">--rm</span> <span class="nt">--name</span> cron <span class="nt">-e</span> <span class="nv">CUSTOM_ENV_VAR</span><span class="o">=</span>foobar <span class="nt">-v</span> <span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>/crontab:/etc/crontab analogj/cron
</code></pre></div></div>

<p>You should see output like the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>foobar
Tue Apr 27 14:31:00 UTC 2021
</code></pre></div></div>

<h1 id="fin">Fin</h1>

<p>I’ve put together a working example of dockerized <code class="language-plaintext highlighter-rouge">cron</code> for multiple distros:</p>

<div class="github-widget" data-repo="AnalogJ/docker-cron"></div>

<h2 id="references">References</h2>
<ul>
  <li>https://hynek.me/articles/docker-signals/</li>
  <li>https://stackoverflow.com/questions/37458287/how-to-run-a-cron-job-inside-a-docker-container</li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Traefik v2 - Advanced Configuration</title>
	  <link>/traefik-advanced-config</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2020-05-28T00:37:09-05:00</pubDate>
	  <guid>/traefik-advanced-config</guid>
	  <description><![CDATA[
	     <blockquote>
  <p>Traefik is the leading open source reverse proxy and load balancer for HTTP and TCP-based applications that is easy,
dynamic, automatic, fast, full-featured, production proven, provides metrics, and integrates with every major cluster technology
      https://containo.us/traefik/</p>
</blockquote>

<p>Still not sure what Traefik is? Basically it’s a load balancer &amp; reverse proxy that integrates with docker/kubernetes to automatically
route requests to your containers, with very little configuration.</p>

<p>The release of Traefik v2, while adding tons of features, also completely threw away backwards compatibility, meaning that
 the documentation and guides you can find on the internet are basically useless.
It doesn’t help that the auto-magic configuration only works for toy examples. To do anything complicated requires some actual configuration.</p>

<p>This guide assumes you’re somewhat familiar with Traefik, and you’re interested in adding some of the advanced features mentioned in the Table of Contents.</p>

<h2 id="requirements">Requirements</h2>

<ul>
  <li>Docker</li>
  <li>A custom domain to assign to Traefik, or a <a href="https://blog.thesparktree.com/local-development-with-wildcard-dns">fake domain (.lan) configured for wildcard local development</a></li>
</ul>

<h2 id="base-traefik-docker-compose">Base Traefik Docker-Compose</h2>

<p>Before we start working with the advanced features of Traefik, lets get a simple example working.
We’ll use this example as the base for any changes necessary to enable an advanced Traefik feature.</p>

<ul>
  <li>
    <p>First, we need to create a shared Docker network. Docker Compose (which we’ll be using in the following examples) will create your container(s)
but it will also create a docker network specifically for containers defined in the compose file. This is fine until
you notice that traefik is unable to route to containers defined in other <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> files, or started manually via <code class="language-plaintext highlighter-rouge">docker run</code>
To solve this, we’ll need to create a shared docker network using <code class="language-plaintext highlighter-rouge">docker network create traefik</code> first.</p>
  </li>
  <li>
    <p>Next, lets create a new folder and a <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file. In the subsequent examples, all differences from this config will be bolded.</p>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">2'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">traefik</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">traefik:v2.2</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="c1"># The HTTP port</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">80:80"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="c1"># For Traefik's automated config to work, the docker socket needs to be</span>
      <span class="c1"># mounted. There are some security implications to this.</span>
      <span class="c1"># See https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface</span>
      <span class="c1"># and https://docs.traefik.io/providers/docker/#docker-api-access</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">/var/run/docker.sock:/var/run/docker.sock:ro"</span>
    <span class="na">command</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">--providers.docker</span>
      <span class="pi">-</span> <span class="s">--entrypoints.web.address=:80</span>
      <span class="pi">-</span> <span class="s">--providers.docker.network=traefik</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">traefik</span>

<span class="c1"># Use our previously created `traefik` docker network, so that we can route to</span>
<span class="c1"># containers that are created in external docker-compose files and manually via</span>
<span class="c1"># `docker run`</span>
<span class="na">networks</span><span class="pi">:</span>
  <span class="na">traefik</span><span class="pi">:</span>
    <span class="na">external</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>

<h2 id="webui-dashboard">WebUI Dashboard</h2>

<p>First, lets start by enabling the built in Traefik dashboard. This dashboard is useful for debugging as we enable other
advanced features, however you’ll want to ensure that it’s disabled in production.</p>

<pre><code class="yaml">
version: '2'
services:
  traefik:
    image: traefik:v2.2
    ports:
      - "80:80"
      <b># The Web UI (enabled by --api.insecure=true)</b>
      <b>- "8080:8080"</b>
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
    command:
      - --providers.docker
      - --entrypoints.web.address=:80
      - --providers.docker.network=traefik
      <b>- --api.insecure=true</b>
    labels:
      <b>- 'traefik.http.routers.traefik.rule=Host(`traefik.example.com`)'</b>
      <b>- 'traefik.http.routers.traefik.service=api@internal'</b>
    networks:
      - traefik
networks:
  traefik:
    external: true
</code></pre>

<p>In a browser, just open up <code class="language-plaintext highlighter-rouge">http://traefik.example.com</code> or the domain name you specified in the <code class="language-plaintext highlighter-rouge">traefik.http.routers.traefik.rule</code> label.
You should see the following dashboard:</p>

<p><img src="https://blog.thesparktree.com/assets/images/traefik/traefik-dashboard.png" alt="traefik dashboard" style="max-height: 500px;" /></p>

<h2 id="automatic-subdomain-routing">Automatic Subdomain Routing</h2>

<p>One of the most useful things about Traefik is its ability to dynamically route traffic to containers.
Rather than have to explicitly assign a domain or subdomain for each container, you can tell Traefik to use the container name
(or service name in a docker-compose file) prepended to a domain name for dynamic routing. eg. <code class="language-plaintext highlighter-rouge">container_name.example.com</code></p>

<pre><code class="yaml">
version: '2'
services:
  traefik:
    image: traefik:v2.2
    ports:
      - "80:80"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
    command:
      - --providers.docker
      - --entrypoints.web.address=:80
      - --providers.docker.network=traefik
      <b>- '--providers.docker.defaultRule=Host(`{{ normalize .Name }}.example.com`)'</b>
    networks:
      - traefik
networks:
  traefik:
    external: true
</code></pre>

<p>Next, lets start up a Docker container running the actual server that we want to route to.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="se">\</span>
    <span class="nt">--rm</span> <span class="se">\</span>
    <span class="nt">--label</span> <span class="s1">'traefik.http.services.foo.loadbalancer.server.port=80'</span> <span class="se">\</span>
    <span class="nt">--name</span> <span class="s1">'foo'</span> <span class="se">\</span>
    <span class="nt">--network</span><span class="o">=</span>traefik <span class="se">\</span>
    containous/whoami

</code></pre></div></div>

<p>Whenever a container starts Traefik will interpolate the <code class="language-plaintext highlighter-rouge">defaultRule</code> and configure a router for this container.
In this example, we’ve specified that the container name is <code class="language-plaintext highlighter-rouge">foo</code>, so the container will be accessible at
<code class="language-plaintext highlighter-rouge">foo.example.com</code></p>

<blockquote>
  <p>Note: if your service is running in another docker-compose file, <code class="language-plaintext highlighter-rouge">{{ normalize .Name }}</code> will be interpolated as: <code class="language-plaintext highlighter-rouge">service_name-folder_name</code>,
so your container will be accessible at <code class="language-plaintext highlighter-rouge">service_name-folder_name.example.com</code></p>
</blockquote>

<h3 id="override-subdomain-routing-using-container-labels">Override Subdomain Routing using Container Labels</h3>

<p>You can override the default routing rule (<code class="language-plaintext highlighter-rouge">providers.docker.defaultRule</code>) for your container by adding a <code class="language-plaintext highlighter-rouge">traefik.http.routers.*.rule</code> label.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="se">\</span>
    <span class="nt">--rm</span> <span class="se">\</span>
    <span class="nt">--label</span> <span class="s1">'traefik.http.services.foo.loadbalancer.server.port=80'</span> <span class="se">\</span>
    <span class="nt">--label</span> <span class="s1">'traefik.http.routers.foo.rule=Host(`bar.example.com`)'</span>
    <span class="nt">--name</span> <span class="s1">'foo'</span> <span class="se">\</span>
    <span class="nt">--network</span><span class="o">=</span>traefik <span class="se">\</span>
    containous/whoami

</code></pre></div></div>

<h2 id="restrict-scope">Restrict Scope</h2>
<p>By default Traefik will watch for all containers running on the Docker daemon, and attempt to automatically configure routes and services for each.
If you’d like a litte more control, you can pass the <code class="language-plaintext highlighter-rouge">--providers.docker.exposedByDefault=false</code> CMD argument to the Traefik container and selectively
enable routing for your containers by adding a <code class="language-plaintext highlighter-rouge">traefik.enable=true</code> label.</p>

<pre><code class="yaml">
version: '2'
services:
  traefik:
    image: traefik:v2.2
    ports:
      - "80:80"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
    command:
      - --providers.docker
      - --entrypoints.web.address=:80
      - --providers.docker.network=traefik
      - '--providers.docker.defaultRule=Host(`{{ normalize .Name }}.example.com`)'
      <b>- '--providers.docker.exposedByDefault=false'</b>
    networks:
      - traefik

  hellosvc:
    image: containous/whoami
    labels:
      <b>- traefik.enable=true</b>
    networks:
      - traefik
networks:
  traefik:
    external: true
</code></pre>

<p>As I mentioned earlier, <code class="language-plaintext highlighter-rouge">normalize .Name</code> will be interpolated as <code class="language-plaintext highlighter-rouge">service_name-folder_name</code> for containers started via docker-compose.
So my Hello-World test container will be accessible as <code class="language-plaintext highlighter-rouge">hellosvc-tmp.example.com</code> on my local machine.</p>

<h2 id="automated-ssl-certificates-using-letsencrypt-dns-integration">Automated SSL Certificates using LetsEncrypt DNS Integration</h2>
<p>Next, lets look at how to securely access Traefik managed containers over SSL using LetsEncrypt certificates.</p>

<p>The great thing about this setup is that Traefik will automatically request and renew the SSL certificate for you, even if your
site is not accessible on the public internet.</p>

<pre><code class="yaml">
version: '2'
services:
  traefik:
    image: traefik:v2.2
    ports:
      - "80:80"
      <b># The HTTPS port</b>
      <b>- "443:443"</b>
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      <b># It's a good practice to persist the Letsencrypt certificates so that they don't change if the Traefik container needs to be restarted.</b>
      <b>- "./letsencrypt:/letsencrypt"</b>
    command:
      - --providers.docker
      - --entrypoints.web.address=:80
      <b>- --entrypoints.websecure.address=:443</b>
      - --providers.docker.network=traefik
      - '--providers.docker.defaultRule=Host(`{{ normalize .Name }}.example.com`)'
      <b># We're going to use the DNS challenge since it allows us to generate</b>
      <b># certificates for intranet/lan sites as well</b>
      <b>- "--certificatesresolvers.mydnschallenge.acme.dnschallenge=true"</b>
      <b># We're using cloudflare for this example, but many DNS providers are</b>
      <b># supported: https://docs.traefik.io/https/acme/#providers </b>
      <b>- "--certificatesresolvers.mydnschallenge.acme.dnschallenge.provider=cloudflare"</b>
      <b>- "--certificatesresolvers.mydnschallenge.acme.email=postmaster@example.com"</b>
      <b>- "--certificatesresolvers.mydnschallenge.acme.storage=/letsencrypt/acme.json"</b>
    environment:
      <b># We need to provide credentials to our DNS provider.</b>
      <b># See https://docs.traefik.io/https/acme/#providers </b>
      <b>- "CF_DNS_API_TOKEN=XXXXXXXXX"</b>
      <b>- "CF_ZONE_API_TOKEN=XXXXXXXXXX"</b>
    networks:
      - traefik

  hellosvc:
    image: containous/whoami
    labels:
      <b>- traefik.http.routers.hellosvc.entrypoints=websecure</b>
      <b>- 'traefik.http.routers.hellosvc.tls.certresolver=mydnschallenge'</b>
    networks:
      - traefik
networks:
  traefik:
    external: true
</code></pre>

<p>Now we can visit our Hello World container by visiting <code class="language-plaintext highlighter-rouge">https://hellosvc-tmp.example.com</code>.</p>

<p><img src="https://blog.thesparktree.com/assets/images/traefik/traefik-letsencrypt.jpg" alt="letsencrypt ssl certificate" style="max-height: 500px;" /></p>

<p>Note: Traefik requires additional configuration to automatically redirect HTTP to HTTPS. See the instructions in the next section.</p>

<h3 id="automatically-redirect-http---https">Automatically Redirect HTTP -&gt; HTTPS.</h3>

<pre><code class="yaml">
version: '2'
services:
  traefik:
    image: traefik:v2.2
    ports:
      - "80:80"
      # The HTTPS port
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "./letsencrypt:/letsencrypt"
    command:
      - --providers.docker
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      <b>- --entrypoints.web.http.redirections.entryPoint.to=websecure</b>
      <b>- --entrypoints.web.http.redirections.entryPoint.scheme=https</b>
      - --providers.docker.network=traefik
      - '--providers.docker.defaultRule=Host(`{{ normalize .Name }}.example.com`)'
      - "--certificatesresolvers.mydnschallenge.acme.dnschallenge=true"
      - "--certificatesresolvers.mydnschallenge.acme.dnschallenge.provider=cloudflare"
      - "--certificatesresolvers.mydnschallenge.acme.email=postmaster@example.com"
      - "--certificatesresolvers.mydnschallenge.acme.storage=/letsencrypt/acme.json"

    environment:
      - "CF_DNS_API_TOKEN=XXXXXXXXX"
      - "CF_ZONE_API_TOKEN=XXXXXXXXXX"
    networks:
      - traefik

  hellosvc:
    image: containous/whoami
    labels:
      - traefik.http.routers.hellosvc.entrypoints=websecure
      - 'traefik.http.routers.hellosvc.tls.certresolver=mydnschallenge'
    networks:
      - traefik
networks:
  traefik:
    external: true
</code></pre>

<p>Note, the <code class="language-plaintext highlighter-rouge">--entrypoints.web.http.redirections.entryPoint.*</code> <code class="language-plaintext highlighter-rouge">command line flags</code> are only available in Traefik v2.2+. If you need HTTP to HTTPS
redirection for Traefik v2.0 or v2.1, you’ll need to add the following <code class="language-plaintext highlighter-rouge">labels</code> instead:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>traefik:
  ....
  labels:
    - traefik.http.routers.https-redirect.entrypoints=web
    - traefik.http.routers.https-redirect.rule=HostRegexp(`{any:.*}`)
    - traefik.http.routers.https-redirect.middlewares=https-only
    - traefik.http.middlewares.https-only.redirectscheme.scheme=https
</code></pre></div></div>

<h2 id="2fa-sso-and-saml">2FA, SSO and SAML</h2>

<p>Traefik supports using an external service to check for credentials. This external service can then be used to enable
single sign on (SSO) for your apps, including 2FA and/or SAML.</p>

<p><img src="https://blog.thesparktree.com/assets/images/traefik/traefik-authforward.png" alt="Traefik external service" style="max-height: 500px;" /></p>

<p>In this example, I’ll be using <a href="https://github.com/authelia/authelia">Authelia</a> to enable SSO, but please note that Authelia does
not support SAML, only 2FA and Forward Auth.</p>

<p>Authelia requires HTTPS, so we’ll base our Traefik configuration on the previous example (Traefik with Letsencrypt certificates &amp; Http to Https redirects)</p>

<pre><code class="yaml">
version: '2'
services:
  traefik:
    image: traefik:v2.2
    ports:
      - "80:80"
      # The HTTPS port
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "./letsencrypt:/letsencrypt"
    command:
      - --providers.docker
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      - --entrypoints.web.http.redirections.entryPoint.to=websecure
      - --entrypoints.web.http.redirections.entryPoint.scheme=https
      - --providers.docker.network=traefik
      - '--providers.docker.defaultRule=Host(`{{ normalize .Name }}.example.com`)'
      - "--certificatesresolvers.mydnschallenge.acme.dnschallenge=true"
      - "--certificatesresolvers.mydnschallenge.acme.dnschallenge.provider=cloudflare"
      - "--certificatesresolvers.mydnschallenge.acme.email=postmaster@example.com"
      - "--certificatesresolvers.mydnschallenge.acme.storage=/letsencrypt/acme.json"

    environment:
      - "CF_DNS_API_TOKEN=XXXXXXXXX"
      - "CF_ZONE_API_TOKEN=XXXXXXXXXX"
    networks:
      - traefik

  authelia:
    image: authelia/authelia
    volumes:
      - './authelia/configuration.yml:/etc/authelia/configuration.yml:ro'
      - './authelia/users_database.yml:/etc/authelia/users_database.yml:ro'
      - './authelia/data:/etc/authelia/data:rw'
    environment:
      - 'TZ=America/Los_Angeles'
    labels:
      - 'traefik.http.services.authelia.loadbalancer.server.port=9091'
      - 'traefik.http.routers.authelia.rule=Host(`login.example.com`)'
      - 'traefik.http.routers.authelia.entrypoints=websecure'
      - 'traefik.http.routers.authelia.tls.certresolver=mydnschallenge'
    networks:
      - traefik

  hellosvc:
    image: containous/whoami
    labels:
      - traefik.http.routers.hellosvc.entrypoints=websecure
      - 'traefik.http.routers.hellosvc.tls.certresolver=mydnschallenge'
      <b>- 'traefik.http.routers.hellosvc.middlewares=authme'</b>

      <b># this forwardauth.address is complex but incredibly important.</b>
      <b># http://authelia:9091 is the internal routable container name.</b>
      <b># https://login.example.com is the external url for authelia </b>
      <b>- 'traefik.http.middlewares.authme.forwardauth.address=http://authelia:9091/api/verify?rd=https://login.example.com/'</b>
      <b>- 'traefik.http.middlewares.authme.forwardauth.trustforwardheader=true'</b>
      <b>- 'traefik.http.middlewares.authme.forwardauth.authresponseheaders=X-Forwarded-User'</b>
    networks:
      - traefik

networks:
  traefik:
    external: true
</code></pre>

<p>In the above <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file, under the <code class="language-plaintext highlighter-rouge">authelia</code> service, 2 config files are referenced <code class="language-plaintext highlighter-rouge">configuration.yml</code> and <code class="language-plaintext highlighter-rouge">users_database.yml</code>.</p>

<p><code class="language-plaintext highlighter-rouge">configuration.yml</code> is the configuration file for Authelia. Here’s an example of what that file looks like. You will need ensure that
all references to the <code class="language-plaintext highlighter-rouge">example.com</code> domain are replaced with your chosen (sub)domain.</p>

<p>See <a href="https://github.com/authelia/authelia/blob/master/config.template.yml">config.template.yml on github</a> for a comprehensive list of options.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">###############################################################</span>
<span class="c1">#                   Authelia configuration                    #</span>
<span class="c1">###############################################################</span>

<span class="c1"># The host and port to listen on</span>
<span class="na">host</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
<span class="na">port</span><span class="pi">:</span> <span class="m">9091</span>

<span class="c1"># Level of verbosity for logs: info, debug, trace</span>
<span class="na">log_level</span><span class="pi">:</span> <span class="s">info</span>

<span class="c1"># The secret used to generate JWT tokens when validating user identity by</span>
<span class="c1"># email confirmation.</span>
<span class="na">jwt_secret</span><span class="pi">:</span> <span class="s">change_this_secret</span>

<span class="c1"># Default redirection URL</span>
<span class="c1">#</span>
<span class="c1"># If user tries to authenticate without any referer, Authelia</span>
<span class="c1"># does not know where to redirect the user to at the end of the</span>
<span class="c1"># authentication process.</span>
<span class="c1"># This parameter allows you to specify the default redirection</span>
<span class="c1"># URL Authelia will use in such a case.</span>
<span class="c1">#</span>
<span class="c1"># Note: this parameter is optional. If not provided, user won't</span>
<span class="c1"># be redirected upon successful authentication.</span>
<span class="na">default_redirection_url</span><span class="pi">:</span> <span class="s">http://example.com/</span>

<span class="c1"># TOTP Issuer Name</span>
<span class="c1">#</span>
<span class="c1"># This will be the issuer name displayed in Google Authenticator</span>
<span class="c1"># See: https://github.com/google/google-authenticator/wiki/Key-Uri-Format for more info on issuer names</span>
<span class="na">totp</span><span class="pi">:</span>
  <span class="na">issuer</span><span class="pi">:</span> <span class="s">authelia.com</span>

<span class="c1"># Duo Push API</span>
<span class="c1">#</span>
<span class="c1"># Parameters used to contact the Duo API. Those are generated when you protect an application</span>
<span class="c1"># of type "Partner Auth API" in the management panel.</span>
<span class="c1"># duo_api:</span>
<span class="c1">#   hostname: api-123456789.example.com</span>
<span class="c1">#   integration_key: ABCDEF</span>
<span class="c1">#   secret_key: 1234567890abcdefghifjkl</span>

<span class="c1"># The authentication backend to use for verifying user passwords</span>
<span class="c1"># and retrieve information such as email address and groups</span>
<span class="c1"># users belong to.</span>
<span class="c1">#</span>
<span class="c1"># There are two supported backends: `ldap` and `file`.</span>
<span class="na">authentication_backend</span><span class="pi">:</span>

  <span class="c1"># File backend configuration.</span>
  <span class="c1">#</span>
  <span class="c1"># With this backend, the users database is stored in a file</span>
  <span class="c1"># which is updated when users reset their passwords.</span>
  <span class="c1"># Therefore, this backend is meant to be used in a dev environment</span>
  <span class="c1"># and not in production since it prevents Authelia to be scaled to</span>
  <span class="c1"># more than one instance.</span>
  <span class="c1">#</span>
  <span class="na">file</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/authelia/users_database.yml</span>

<span class="c1"># Access Control</span>
<span class="c1">#</span>
<span class="c1"># Access control is a list of rules defining the authorizations applied for one</span>
<span class="c1"># resource to users or group of users.</span>
<span class="c1">#</span>
<span class="c1"># If 'access_control' is not defined, ACL rules are disabled and the `bypass`</span>
<span class="c1"># rule is applied, i.e., access is allowed to anyone. Otherwise restrictions follow</span>
<span class="c1"># the rules defined.</span>
<span class="c1">#</span>
<span class="c1"># Note: One can use the wildcard * to match any subdomain.</span>
<span class="c1"># It must stand at the beginning of the pattern. (example: *.mydomain.com)</span>
<span class="c1">#</span>
<span class="c1"># Note: You must put patterns containing wildcards between simple quotes for the YAML</span>
<span class="c1"># to be syntactically correct.</span>
<span class="c1">#</span>
<span class="c1"># Definition: A `rule` is an object with the following keys: `domain`, `subject`,</span>
<span class="c1"># `policy` and `resources`.</span>
<span class="c1">#</span>
<span class="c1"># - `domain` defines which domain or set of domains the rule applies to.</span>
<span class="c1">#</span>
<span class="c1"># - `subject` defines the subject to apply authorizations to. This parameter is</span>
<span class="c1">#    optional and matching any user if not provided. If provided, the parameter</span>
<span class="c1">#    represents either a user or a group. It should be of the form 'user:&lt;username&gt;'</span>
<span class="c1">#    or 'group:&lt;groupname&gt;'.</span>
<span class="c1">#</span>
<span class="c1"># - `policy` is the policy to apply to resources. It must be either `bypass`,</span>
<span class="c1">#   `one_factor`, `two_factor` or `deny`.</span>
<span class="c1">#</span>
<span class="c1"># - `resources` is a list of regular expressions that matches a set of resources to</span>
<span class="c1">#    apply the policy to. This parameter is optional and matches any resource if not</span>
<span class="c1">#    provided.</span>
<span class="c1">#</span>
<span class="c1"># Note: the order of the rules is important. The first policy matching</span>
<span class="c1"># (domain, resource, subject) applies.</span>
<span class="na">access_control</span><span class="pi">:</span>
  <span class="c1"># Default policy can either be `bypass`, `one_factor`, `two_factor` or `deny`.</span>
  <span class="c1"># It is the policy applied to any resource if there is no policy to be applied</span>
  <span class="c1"># to the user.</span>
  <span class="na">default_policy</span><span class="pi">:</span> <span class="s">deny</span>

  <span class="na">rules</span><span class="pi">:</span>
    <span class="c1"># Rules applied to everyone</span>

    <span class="pi">-</span> <span class="na">domain</span><span class="pi">:</span> <span class="s2">"</span><span class="s">*.example.com"</span>
      <span class="na">policy</span><span class="pi">:</span> <span class="s">one_factor</span>

<span class="c1"># Configuration of session cookies</span>
<span class="c1">#</span>
<span class="c1"># The session cookies identify the user once logged in.</span>
<span class="na">session</span><span class="pi">:</span>
  <span class="c1"># The name of the session cookie. (default: authelia_session).</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">authelia_session</span>

  <span class="c1"># The secret to encrypt the session cookie.</span>
  <span class="na">secret</span><span class="pi">:</span> <span class="s">change_this_secret</span>

  <span class="c1"># The time in seconds before the cookie expires and session is reset.</span>
  <span class="na">expiration</span><span class="pi">:</span> <span class="m">3600</span> <span class="c1"># 1 hour</span>

  <span class="c1"># The inactivity time in seconds before the session is reset.</span>
  <span class="na">inactivity</span><span class="pi">:</span> <span class="m">300</span> <span class="c1"># 5 minutes</span>

  <span class="c1"># The domain to protect.</span>
  <span class="c1"># Note: the authenticator must also be in that domain. If empty, the cookie</span>
  <span class="c1"># is restricted to the subdomain of the issuer.</span>
  <span class="na">domain</span><span class="pi">:</span> <span class="s">example.com</span>

<span class="c1"># Configuration of the authentication regulation mechanism.</span>
<span class="c1">#</span>
<span class="c1"># This mechanism prevents attackers from brute forcing the first factor.</span>
<span class="c1"># It bans the user if too many attempts are done in a short period of</span>
<span class="c1"># time.</span>
<span class="na">regulation</span><span class="pi">:</span>
  <span class="c1"># The number of failed login attempts before user is banned.</span>
  <span class="c1"># Set it to 0 to disable regulation.</span>
  <span class="na">max_retries</span><span class="pi">:</span> <span class="m">3</span>

  <span class="c1"># The time range during which the user can attempt login before being banned.</span>
  <span class="c1"># The user is banned if the authentication failed `max_retries` times in a `find_time` seconds window.</span>
  <span class="na">find_time</span><span class="pi">:</span> <span class="m">120</span>

  <span class="c1"># The length of time before a banned user can login again.</span>
  <span class="na">ban_time</span><span class="pi">:</span> <span class="m">300</span>

<span class="c1"># Configuration of the storage backend used to store data and secrets.</span>
<span class="c1">#</span>
<span class="c1"># You must use only an available configuration: local, sql</span>
<span class="na">storage</span><span class="pi">:</span>
  <span class="c1"># The directory where the DB files will be saved</span>
  <span class="na">local</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/authelia/data/db.sqlite3</span>

<span class="c1"># Configuration of the notification system.</span>
<span class="c1">#</span>
<span class="c1"># Notifications are sent to users when they require a password reset, a u2f</span>
<span class="c1"># registration or a TOTP registration.</span>
<span class="c1"># Use only an available configuration: filesystem, gmail</span>
<span class="na">notifier</span><span class="pi">:</span>
  <span class="c1"># For testing purpose, notifications can be sent in a file</span>
  <span class="na">filesystem</span><span class="pi">:</span>
    <span class="na">filename</span><span class="pi">:</span> <span class="s">/etc/authelia/data/notification.txt</span>

  <span class="c1"># Sending an email using a Gmail account is as simple as the next section.</span>
  <span class="c1"># You need to create an app password by following: https://support.google.com/accounts/answer/185833?hl=en</span>
  <span class="c1">## smtp:</span>
  <span class="c1">##   username: myaccount@gmail.com</span>
  <span class="c1">##   password: yourapppassword</span>
  <span class="c1">##   sender: admin@example.com</span>
  <span class="c1">##   host: smtp.gmail.com</span>
  <span class="c1">##   port: 587</span>
</code></pre></div></div>

<p>In this example we use a hard coded user database, defined in <code class="language-plaintext highlighter-rouge">users_database.yml</code>. Authelia also supports LDAP integration.</p>

<p>See the <a href="https://docs.authelia.com/configuration/authentication/file.html#password-hash-algorithm-tuning">password-hash-algorithm-tuning</a> documentation for more information.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">users</span><span class="pi">:</span>
  <span class="na">testuser</span><span class="pi">:</span> <span class="c1">## I have set the password below to 'test' for you</span>
    <span class="na">password</span><span class="pi">:</span> <span class="s1">'</span><span class="s">{CRYPT}$6$rounds=500000$Bui4ldW5hXOI9qwJ$IUHQPCusUKpTs/OrfE9UuGb1Giqaa5OZA.mqIpH.Hh8RGFsEBHViCwQDx6DfkGUiF60pqNubFBugfTvCJIDNw1'</span>
    <span class="na">email</span><span class="pi">:</span> <span class="s">your@email.address</span>
    <span class="na">groups</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">admins</span>
      <span class="pi">-</span> <span class="s">dev</span>
</code></pre></div></div>

<p>Once you start your docker-compose file and try to access the <code class="language-plaintext highlighter-rouge">hellosvc</code> url, you’ll be redirected automatically to the Authelia login page.</p>

<p><img src="https://blog.thesparktree.com/assets/images/traefik/traefik-authelia.png" alt="Authelia login page" style="max-height: 500px;" /></p>

<h1 id="fin">Fin.</h1>

<p>As you can see, Traefik v2 is pretty powerful, if a bit verbose with its configuration syntax. With its native docker
integration, support for LetsEncrypt and SSO, it’s become a staple of my docker based server environments.</p>


	  ]]></description>
	</item>

	<item>
	  <title>Docker Hub - Matrix Builds and Tagging using Build Args</title>
	  <link>/docker-hub-matrix-builds</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2019-09-12T04:19:33-05:00</pubDate>
	  <guid>/docker-hub-matrix-builds</guid>
	  <description><![CDATA[
	     <p>If you’re a heavy user of Docker, you’re already intimately familiar with Docker Hub, the official Docker Image registry.
One of the best things about Docker Hub is it’s support for Automated Builds, which is where Docker Hub will watch a
Git repository for changes, and automatically build your Docker images whenever you make a new commit.</p>

<p>This works great for most simple use cases (and even some complex ones), but occasionally you’ll wish you had a bit more control
over the Docker Hub image build process.</p>

<p>That’s where Docker’s <a href="https://docs.docker.com/docker-hub/builds/advanced/">Advanced options for Autobuild and Autotest</a>
guide comes in. While it’s not quite a turn key solution, Docker Hub allows you to override the <code class="language-plaintext highlighter-rouge">test</code>, <code class="language-plaintext highlighter-rouge">build</code> and <code class="language-plaintext highlighter-rouge">push</code>
stages completely, as well as run arbitrary code <code class="language-plaintext highlighter-rouge">pre</code> and <code class="language-plaintext highlighter-rouge">post</code> each of those stages.</p>

<p>As always, here’s a Github repo with working code if you want to skip ahead:</p>

<div class="github-widget" data-repo="AnalogJ/docker-hub-matrix-builds"></div>

<h2 id="goal">Goal</h2>

<p>So what’s the point? If Docker Hub works fine for most people, what’s an actual use case for these Advanced Options?</p>

<p>Lets say you have developed a tool, and you would like to distribute it as a Docker image. The first problem is that you’d
like to provide Docker images based on a handful of different OS’s. <code class="language-plaintext highlighter-rouge">ubuntu</code>, <code class="language-plaintext highlighter-rouge">centos6</code>, <code class="language-plaintext highlighter-rouge">centos7</code> and <code class="language-plaintext highlighter-rouge">alpine</code>
Simple enough, just write a handful of Dockerfiles, and use the <code class="language-plaintext highlighter-rouge">FROM</code> instruction.
But lets say that you also need to provide multiple versions of your tool, and each of those must also be distributed as a
Docker Image based on different OS’s.</p>

<p>Now the number of Dockerfiles you need to maintain has increased significantly. If you’re familiar with Jenkins, this would
be perfect for a “Matrix Project”.</p>

<p>Here’s what our Docker naming scheme might look like:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>ubuntu</th>
      <th>centos6</th>
      <th>centos7</th>
      <th>alpine</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>v1.x</td>
      <td>v1-ubuntu</td>
      <td>v1-centos6</td>
      <td>v1-centos7</td>
      <td>v1-alpine</td>
    </tr>
    <tr>
      <td>v2.x</td>
      <td>v2-ubuntu</td>
      <td>v2-centos6</td>
      <td>v2-centos7</td>
      <td>v2-alpine</td>
    </tr>
    <tr>
      <td>v3.x</td>
      <td>v3-ubuntu</td>
      <td>v3-centos6</td>
      <td>v3-centos7</td>
      <td>v3-alpine</td>
    </tr>
  </tbody>
</table>

<p>As our software grows, you could image other axises being added: architectures, software runtimes, etc.</p>

<h2 id="build-arguments">Build Arguments</h2>

<p>Alright, so the first part of the solution is just making use of Dockerfile templating, also known as <a href="https://docs.docker.com/engine/reference/commandline/build/#set-build-time-variables---build-arg">build arguments</a></p>

<p>To keep the number of Dockerfiles to the minimum, we need to pick an axes that minimizes the number of changes required.
In this example we’ll choose to create a separate Dockerfile for each OS, reusing it for each branch of our software.</p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu</span>
<span class="k">ARG</span><span class="s"> software_version</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> &lt;dependencies&gt; <span class="se">\
</span>    ... <span class="se">\
</span>    curl <span class="nt">-o</span> /usr/bin/myapp https://www.company.com/<span class="k">${</span><span class="nv">software_version</span><span class="k">}</span>/myapp-<span class="k">${</span><span class="nv">software_version</span><span class="k">}</span>

</code></pre></div></div>

<p>Now we can reuse this single Dockerfile to build 3 Docker images, running 3 different versions of our software:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build -f ubuntu/Dockerfile --build-arg software_version=v1.0 -t v1-ubuntu .
docker build -f ubuntu/Dockerfile --build-arg software_version=v2.1 -t v2-ubuntu .
docker build -f ubuntu/Dockerfile --build-arg software_version=v3.7 -t v3-ubuntu .
</code></pre></div></div>

<h2 id="project-structure">Project Structure</h2>
<p>Looks great so far, but Docker Hub doesn’t support configuring Build Arguments though their web ui. So we’ll need to use the
“Advanced options for Autobuild” documentation to override it.</p>

<p>At this point our project repository probably looks something like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>project/
├── ubuntu/
│   └── Dockerfile
├── centos6/
│   └── Dockerfile
├── centos7/
│   └── Dockerfile
...
</code></pre></div></div>

<p>Docker Hub requires that the hook override directory is located as a sibling to the Dockerfile.
To keep our repository DRY, we’ll instead create a <code class="language-plaintext highlighter-rouge">hook</code> directory at the top level, and symlink our <code class="language-plaintext highlighter-rouge">build</code> and <code class="language-plaintext highlighter-rouge">push</code>
scripts into a hooks directory beside each Dockerfile. We’ll also create an empty <code class="language-plaintext highlighter-rouge">software-versions.txt</code> file in the project root,
which we’ll use to store the versions of our software that needs to be automatically build. We’ll discuss this further in the next section.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>project/
├── software-versions.txt
├── hooks/
│   ├── build
│   └── push
├── ubuntu/
│   ├── hooks/
│   │   ├── build (symlink)
│   │   └── push (symlink)
│   └── Dockerfile
├── centos6/
│   ├── hooks/
│   │   ├── build (symlink)
│   │   └── push (symlink)
│   └── Dockerfile
├── centos7/
│   ├── hooks/
│   │   ├── build (symlink)
│   │   └── push (symlink)
│   └── Dockerfile
...
</code></pre></div></div>

<p>Now that we have our project organized in a way that Docker Hub expects, lets populate our override scripts</p>

<h2 id="docker-hub-hook-override-scripts">Docker Hub Hook Override Scripts</h2>

<p>Docker Hub provides the following environmental variables which are available to us in the logic of our scripts.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">SOURCE_BRANCH</code>: the name of the branch or the tag that is currently being tested.</li>
  <li><code class="language-plaintext highlighter-rouge">SOURCE_COMMIT</code>: the SHA1 hash of the commit being tested.</li>
  <li><code class="language-plaintext highlighter-rouge">COMMIT_MSG</code>: the message from the commit being tested and built.</li>
  <li><code class="language-plaintext highlighter-rouge">DOCKER_REPO</code>: the name of the Docker repository being built.</li>
  <li><code class="language-plaintext highlighter-rouge">DOCKERFILE_PATH</code>: the dockerfile currently being built.</li>
  <li><code class="language-plaintext highlighter-rouge">DOCKER_TAG</code>: the Docker repository tag being built.</li>
  <li><code class="language-plaintext highlighter-rouge">IMAGE_NAME</code>: the name and tag of the Docker repository being built. (This variable is a combination of <code class="language-plaintext highlighter-rouge">DOCKER_REPO</code>:<code class="language-plaintext highlighter-rouge">DOCKER_TAG</code>.)</li>
</ul>

<p>The following is a simplified version of a <code class="language-plaintext highlighter-rouge">build</code> hook script that we can use to override the <code class="language-plaintext highlighter-rouge">build</code> step on Docker Hub.
Keep in mind that this script is missing some error handling for readability reasons.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c">###############################################################################</span>
<span class="c"># WARNING</span>
<span class="c"># This is a symlinked file. The original lives at hooks/build in this repository</span>
<span class="c">###############################################################################</span>

<span class="c"># original docker build command</span>
<span class="nb">echo</span> <span class="s2">"overwriting docker build -f </span><span class="nv">$DOCKERFILE_PATH</span><span class="s2"> -t </span><span class="nv">$IMAGE_NAME</span><span class="s2"> ."</span>

<span class="nb">cat</span> <span class="s2">"../software-versions.txt"</span> | <span class="k">while </span><span class="nb">read </span>software_version_line
<span class="k">do</span>
        <span class="c"># The new image tag will include the version of our software, prefixed to the os image we're currently building</span>
        <span class="nv">IMAGE_TAG</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">DOCKER_REPO</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">software_version_line</span><span class="k">}</span><span class="s2">-</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2">"</span>

        <span class="nb">echo</span> <span class="s2">"docker build -f Dockerfile --build-arg software_version=</span><span class="k">${</span><span class="nv">software_version_line</span><span class="k">}</span><span class="s2"> -t </span><span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span><span class="s2"> ../"</span>
        docker build <span class="nt">-f</span> Dockerfile <span class="nt">--build-arg</span> <span class="nv">software_version</span><span class="o">=</span><span class="k">${</span><span class="nv">software_version_line</span><span class="k">}</span> <span class="nt">-t</span> <span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span> ../
<span class="k">done</span>

</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">push</code> script is similar:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c">###############################################################################</span>
<span class="c"># WARNING</span>
<span class="c"># This is a symlinked file. The original lives at hooks/push in this repository</span>
<span class="c">###############################################################################</span>

<span class="c"># original docker push command</span>
<span class="nb">echo</span> <span class="s2">"overwriting docker push </span><span class="nv">$IMAGE_NAME</span><span class="s2">"</span>

<span class="nb">cat</span> <span class="s2">"../software-versions.txt"</span> | <span class="k">while </span><span class="nb">read </span>software_version_line
<span class="k">do</span>
    <span class="c"># The new image tag will include the version of our software, prefixed to the os image we're currently building</span>
    <span class="nv">IMAGE_TAG</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">DOCKER_REPO</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">software_version_line</span><span class="k">}</span><span class="s2">-</span><span class="k">${</span><span class="nv">DOCKER_TAG</span><span class="k">}</span><span class="s2">"</span>

    <span class="nb">echo</span> <span class="s2">"docker push </span><span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span><span class="s2">"</span>
    docker push <span class="k">${</span><span class="nv">IMAGE_TAG</span><span class="k">}</span>
<span class="k">done</span>

</code></pre></div></div>

<p>You should have noticed the <code class="language-plaintext highlighter-rouge">software-versions.txt</code> above. It’s basically a text file that just contains version numbers for
our <code class="language-plaintext highlighter-rouge">myapp</code> software/binary.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>master
v1.0
v2.1
v3.7
</code></pre></div></div>
<p>This file is then read line-by-line, and each line is passed into a docker build command via <code class="language-plaintext highlighter-rouge">--build-arg</code>. It’s also used as the
version component in the Docker image build tag.</p>

<h2 id="docker-hub-configuration">Docker Hub Configuration</h2>

<p>The final component necessary to successfully build these images is to configure the Docker Hub project correctly.</p>

<p><img src="https://blog.thesparktree.com/assets/images/docker-hub/docker-hub-configuration.png" alt="docker hub configuration" style="max-height: 500px;" /></p>

<h2 id="fin">Fin</h2>

<p>Again, here’s the Github repo with working code (using <code class="language-plaintext highlighter-rouge">jq</code> as our example software tool to be installed):</p>

<div class="github-widget" data-repo="AnalogJ/docker-hub-matrix-builds"></div>

	  ]]></description>
	</item>

	<item>
	  <title>Jenkins Dockerized Slave Cluster - Premise</title>
	  <link>/jenkins-dockerized-slave-cluster</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2018-03-25T04:19:33-05:00</pubDate>
	  <guid>/jenkins-dockerized-slave-cluster</guid>
	  <description><![CDATA[
	     <p>Here’s the premise, we have one or more Jenkins masters running our various jobs, and the server is bottlenecking: the UI is sluggish, and builds are taking longer than normal. The obvious answer is to add slaves. But multiple Jenkins masters, each with their own dedicated slaves is a lot of compute power, which may be idle most of the time, meaning a lot of wasted money and resources.</p>

<p>Wouldn’t it be nice if we could share slave nodes between the masters? Create a cluster of slave nodes and have the various Jenkins masters run their jobs without needing to worry about scheduling or the underlying utilization of the hardware?</p>

<p>Enter buzzword heaven. In the next few posts I’ll be going through all the steps required to build a Dockerized Jenkins slave cluster.</p>

<ul>
  <li>Part 1 - Our cloud provider will be OpenStack, however we’ll be using Terraform for provisioning, so you could easily migrate my tutorial onto Azure/AWS/GCE or Bare Metal. Our foundation will be a half-dozen vanilla CoreOS machines, which you can resize to your needs.</li>
  <li>Part 2 - On top of that we’ll use kubeadm to bootstrap a best-practice Kubernetes cluster in an easy, reasonably secure and extensible way. No complex configuration-management required.</li>
  <li>Part 3 - Finally, we’ll configure our Jenkins masters to communicate with a single Kubernetes cluster. The Jenkins masters will run jobs in a “cloud” that will transparently spin up Docker containers on demand. Once the job finishes the container is destroyed automatically, freeing up those resources for other masters and their jobs.</li>
</ul>

<p>My goal with these posts are to:</p>

<ol>
  <li>Aggregate all the steps in one place. There’s alot of smart people out there who’ve written various guides doing each of these things individually. I want to aggregate all the steps into one, easy to follow along tutorial</li>
  <li>Break each stage up into comprehendible chunks, and clearly explain how they interact with each other. This allows you to modify my tutorial to suit your needs, while still being able to follow along.</li>
  <li>Provide a real code repository, not just snippets out of context. Sometimes the “obvious” glue code isn’t so obvious. A repo you can grep can save a lot of time.</li>
  <li>Write a continiously updated/evergreen guide following modern best practices. Like code, content also rots – especially quick in the devops &amp; docker world. I’ll be keeping this guide as up-to-date as possible. In addition it’s hosted on Github, so you can submit edits to make each post better.</li>
</ol>


	  ]]></description>
	</item>

	<item>
	  <title>Devops for Startups & Small Teams</title>
	  <link>/devops-for-startups</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2017-09-13T04:19:33-05:00</pubDate>
	  <guid>/devops-for-startups</guid>
	  <description><![CDATA[
	     <p>When you’re working on a side-project or at a startup as part of a small focused team, it can be hard to get away from
the heads-down mentality of “just do it”. But sometimes it can be valuable to step back and recognize that a bit of upfront
infrastructure work can save you days or even weeks of time getting your MVP up and running.</p>

<p>The following are the quick and dirty Devops patterns &amp; procedures that I put in place before working on any new system.
I primarily focus on free tools and services because of how cheap I am, so feel free to replace them comparable tools of your choice,
you big spender, you.</p>

<h1 id="before-your-first-line">Before your first line</h1>
<ul>
  <li><strong>Store your code in Git</strong> - <a href="https://github.com/">Github</a> <em>[free open source]</em>/<a href="https://bitbucket.org/">Bitbucket</a>
<em>[free private]</em>/<a href="https://www.gitlab.com">GitLab</a> <em>[free private]</em>  -
there shouldn’t be more to say here other than, store your source in a VCS from day 1.</li>
  <li>Design your app with <strong>multiple environments</strong> in mind. You should be able to switch between Local/Stage/Production development
with no code changes, just configuration changes (via environmental variables or a config file). I love <a href="https://github.com/indexzero/nconf">nconf</a>
for NodeJS, but most languages have something similar.</li>
  <li><strong>Isolate your configuration.</strong> Its probably not necessary to move your configuration into a compeltely separate system yet,
but make sure you can easily if you scale. Sprinkling your configuration in multiple places is just asking for an application re-write.</li>
  <li><strong>Follow a branching pattern.</strong> At it’s simplest it could just be 2 branches, “master” and “develop” or you could go nuts
and follow <a href="http://nvie.com/posts/a-successful-git-branching-model/">gitflow</a>. It doesn’t matter, as long as you follow
the damn thing, and don’t just commit directly to master. Setup branch protection to disable commits to “master”.
This is going to be important later when you start doing Continuous Integration (CI). Bad habits are hard to break.</li>
  <li><strong>Setup CI</strong>. You don’t need to go full throttle with a standalone Jenkins server. Just make sure your code is compiling
in a clean-room environment, that doesn’t include the dozens of apps and libraries you already have installed on your
dev machine. <a href="https://travis-ci.org/">TravisCI</a> <em>[free]</em> and <a href="https://circleci.com">CircleCI</a> <em>[free]</em> are great, and integrate
with Github/Bitbucket. At a bare minimum build your artifacts inside a clean Docker container.</li>
  <li>Setup an <strong>issue tracker/project management board</strong>. <a href="https://waffle.io">Waffle.io</a> <em>[free]</em> is great and integrates with Github,
but you may be able to just get away with <a href="https://help.github.com/articles/creating-a-project-board/">Github Project Boards</a> <em>[free]</em> to start</li>
  <li>Make some Architecture decisions:
    <ul>
      <li>Decide if you can get away with a <a href="https://github.com/myles/awesome-static-generators">static frontend</a> or SPA
  architecture for your front end. If you can, you’ll get infinite scaling of your front-end for almost free.
  Distributing static files is a solved problem–CDN’s have been doing it for years. <a href="https://www.cloudflare.com">CloudFlare</a> <em>[free]</em>
  is your <del>cheapest</del> best friend. Pairing it with Github pages [free] is a poor developer’s dream.</li>
      <li>Can you go Serverless/FAAS for your backend? You no longer need to maintain or monitor hardware, you get infinite*
  scaling out of the box. The tradeoff is that your costs will vary with usage, which can be a good thing for startups.</li>
    </ul>
  </li>
</ul>

<h1 id="before-your-first-staging-environment-deploy">Before your first staging environment deploy</h1>
<ul>
  <li>Have a <strong>unit test suite</strong> - Yeah yeah, TDD. But be honest, when’s the last time you started a project with TDD? Still, you’ll thank
yourself when you come back to your code after 2 weeks, or even just a couple of days. It’s also a pre-req for some of the next points.</li>
  <li><strong>Code Coverage/Code Quality</strong> tools - When I feel that I have an application that can actually run on a server is when I
know I need to take a step back and look at all the things that I missed. Code coverage/quality tools are like a bucket of
cold water, they help stifle that feeling of euphoria that stops you from really digging into your code. A nice UI really helps
and I’m a big fan of <a href="https://coveralls.io/">Coveralls.io</a> <em>[free open source]</em> and <a href="https://codecov.io/">CodeCov</a> <em>[free open source]</em>,
both have great integration with SCM’s and CI platforms.</li>
  <li><strong>Forward your logs</strong> to a centralized logging system (Cloud-watch is fine, if you don’t plan on actually debugging your app.)
<a href="https://www.loggly.com">Loggly</a> <em>[free]</em> is great. Make sure you forward environment data and user data to your log aggregator as well, to give your
logs context.</li>
  <li><strong>Use a CDN</strong> like <a href="https://www.cloudflare.com">CloudFlare</a> <em>[free]</em> in front of your site if you haven’t already. You definitely don’t have the traffic yet
that requires it, but don’t wait until you’re ready to launch. Its time-consuming, error prone and can cause DNS downtime,
even if you don’t misconfigure something. It’s not something you want to leave to the last minute.</li>
  <li><strong>Write documentation/setup instructions</strong> as you start building your Stage environment. Your documentation should always
be relative to Stage, <strong>NOT</strong> Production. You will forget. You will copy and paste from your docs, and you will run a
destructive operation against your production database. <a href="https://np.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/">Cough..</a>
    <ul>
      <li>List all the weird/one-off configuration you had to do to get your staging server working. New accounts on 3rd
  party services, ip whitelisting, database population, you’ll need this checklist when you spin up Production, and
  finding out whats different between Prod and Stage is going to be a huge pain without it. Infrastructure-as-code/Configuration Management
   is your friend here, but may not be enough by itself.</li>
    </ul>
  </li>
  <li><strong>Follow modern infrastructure practices.</strong> <a href="https://www.terraform.io/">Infrastructure-as-code</a> and <a href="https://www.chef.io/chef/">Configuration</a> <a href="https://puppet.com/">Management</a> are buzzwords for a reason.
And they don’t have to be super complicated. You don’t need to design the Mona Lisa of Chef cookbooks. At a bare minimum
make sure that you can spin up a whole environment with the click of a single button. Automation is the key here. You’ll
be doing this a lot more than you’d expect, so take some time and do it right. When you find yourself under the gun, needing
to scale your environment, you’ll be thankful.</li>
  <li><strong>Version your code.</strong> Create releases, tag your software, its incredibly useful when debugging what software your actually
running in different environments. It also makes it much easier to deploy previous versions when you want to do regression
testing, or rollback a broken deployment. Check out something like <a href="https://github.com/AnalogJ/capsulecd">CapsuleCD</a>
which can build, test, tag, merge branches and release your software automatically.</li>
  <li><strong>Setup Continuous Deployments</strong> - If you’re already using a CI platform to test your code, why not automatically deploy your
validated code to your Staging environment? Depending on your application architecture, this may be a bit complicated, but
having your CI tested code deployed to a staging environment automatically is going to drastically improve your development
cadence while still ensuring stability. And if your stability is being effected, prioritize your tests, they’re supposed to
catch 90% of your errors before they even get to a staging env.</li>
</ul>

<h1 id="before-your-first-prod-deploy">Before your first prod deploy</h1>
<ul>
  <li><strong>Automate your backups.</strong> This is probably obvious to everyone, but a backup process without a verified restore process is
useless. Try to setup a weekly backup and restore of your staging environment database. Use the same code/process you would in Production.</li>
  <li>Write a script to <strong>populate your database</strong> with test data. Massive amounts of test data. <a href="https://github.com/marak/Faker.js/">Faker.js</a>
has an API. Check how your Staging environment actually handles real data, not just the toy amounts you’ve thrown in.</li>
</ul>

<h1 id="once-your-application-is-live">Once your application is live</h1>
<ul>
  <li>Track the versions of your <strong>application’s dependencies, and their dependencies</strong>,
<a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down">it’s turtles all the way down</a>. This is to ensure that you know
what software makes up your stack, but also so you can be notified of bug fixes and security issues.</li>
  <li>Make sure you have <strong>monitoring</strong> in place.
    <ul>
      <li><a href="https://www.pingdom.com/free">Pingdom</a> <em>[free]</em> will let notify you if your application is inaccessible externally.</li>
      <li>Track system metrics like CPU and memory load on your servers. <a href="https://newrelic.com/">NewRelic</a> <em>[free]</em>,
  <a href="https://www.librato.com/">Librato</a> <em>[free]</em> and <a href="https://cloud.google.com/stackdriver/">StackDriver</a> <em>[paid]</em> work well.</li>
      <li>Configure a user analytics &amp; monitoring solution like <a href="https://www.google.com/analytics/">Google Analytics</a> <em>[free]</em>. Setup alerts when your traffic
  increases or drops more than 15%.</li>
    </ul>
  </li>
</ul>

<p>This is just my checklist, but I’d love to hear yours. Is there any devopsy related tasks you think I’m missing?</p>

	  ]]></description>
	</item>

	<item>
	  <title>CapsuleCD v2 Released</title>
	  <link>/capsulecd-v2-released</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2017-08-06T04:19:33-05:00</pubDate>
	  <guid>/capsulecd-v2-released</guid>
	  <description><![CDATA[
	     <p><a href="https://github.com/AnalogJ/capsulecd">CapsuleCD</a> is made up of a series of scripts/commands that
make it easy for you to package and release a new version of your library artifact (Ruby gem, Npm package, Chef cookbook.. ) while still following best practices:</p>

<ul>
  <li>bumping <code class="language-plaintext highlighter-rouge">semvar</code> tags</li>
  <li>regenerating any <code class="language-plaintext highlighter-rouge">*.lock</code> files</li>
  <li>validates all dependencies exist and are free of vulnerabilities</li>
  <li>runs unit tests &amp; linters</li>
  <li>uploads versioned artifact to community hosting service (rubygems/supermarket/pypi/etc)</li>
  <li>creating a new git tag</li>
  <li>pushing changes back to source control &amp; creating a release</li>
  <li>and others..</li>
</ul>

<p>While <code class="language-plaintext highlighter-rouge">CapsuleCD</code> <strong>was</strong> a series of scripts, with the release of <strong>v2</strong> that’s no longer the case.</p>

<p><code class="language-plaintext highlighter-rouge">CapsuleCD</code> has been re-written, and is now available as a <a href="https://github.com/AnalogJ/capsulecd/releases">static binary</a>
on <a href="https://github.com/AnalogJ/capsulecd/releases/download/v2.0.10/capsulecd-darwin-amd64"><code class="language-plaintext highlighter-rouge">macOS</code></a> and
<a href="https://github.com/AnalogJ/capsulecd/releases/download/v2.0.10/capsulecd-linux-amd64"><code class="language-plaintext highlighter-rouge">Linux</code></a>
(<code class="language-plaintext highlighter-rouge">Windows</code> and <code class="language-plaintext highlighter-rouge">NuGet</code> support is hopefully coming soon)</p>

<p>You no longer need to worry that the version of Ruby used by your library &amp; <code class="language-plaintext highlighter-rouge">gemspec</code> is different than the
version required by <code class="language-plaintext highlighter-rouge">CapsuleCD</code>. If you maintain any Python or NodeJS libraries, this also means that a Ruby
runtime for just for CapsuleCD is unnecessary. The <code class="language-plaintext highlighter-rouge">CapsuleCD</code> <a href="https://hub.docker.com/r/analogj/capsulecd/tags/">Docker</a>
images for other languages are much slimmer, and based off the standard community images with <a href="https://github.com/AnalogJ/capsulecd-docker">minimal changes</a>.</p>

<p>Releasing a new version of your Ruby library hasn’t changed, it’s as easy as downloading the <a href="https://github.com/AnalogJ/capsulecd/releases">binary</a> and running:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CAPSULE_SCM_GITHUB_ACCESS_TOKEN=123456789ABCDEF \
CAPSULE_SCM_REPO_FULL_NAME=AnalogJ/gem_analogj_test \
CAPSULE_SCM_PULL_REQUEST=4 \
CAPSULE_RUBYGEMS_API_KEY=ASDF12345F \
capsulecd start --scm github --package_type ruby
</code></pre></div></div>

<p>Click below to watch a screencast of <code class="language-plaintext highlighter-rouge">CapuleCD</code> in action:</p>

<p align="center">
<a href="https://analogj.github.io/capsulecd">
  <img alt="CapsuleCD screencast" width="800" src="https://cdn.rawgit.com/AnalogJ/capsulecd/v2.0.10/capsulecd-screencast.png" />
  </a>
</p>

<div class="github-widget" data-repo="AnalogJ/capsulecd"></div>


	  ]]></description>
	</item>

	<item>
	  <title>15 Lessons in Golang</title>
	  <link>/15-lessons-in-golang</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2017-07-31T04:19:33-05:00</pubDate>
	  <guid>/15-lessons-in-golang</guid>
	  <description><![CDATA[
	     <p>Like many developers, I heard a lot of buzz about Golang (or is it Go, I’m still not sure).
In case you’re not familiar with it, it’s an open source language developed by Google.
It mostly caught my interest due to the fact that it’s pitched as a statically typed, compiled modern language.</p>

<p>For a long time that was the extent of my Golang knowledge. I knew I wanted to take a closer look at
it at some point, but I had other priorities. About 4 months ago, I realized the Golang could be the
solution to one of the problems I was facing with <a href="https://github.com/AnalogJ/capsulecd">CapsuleCD</a>, my application for generically automating
package releases for any language (npm, cookbooks, gems, pip, jars, etc).</p>

<div class="github-widget" data-repo="AnalogJ/capsulecd"></div>

<p>The problem was that <a href="https://github.com/AnalogJ/capsulecd">CapsuleCD</a> was a executable distributed in a Ruby gem, which meant that anyone
who wanted to use <code class="language-plaintext highlighter-rouge">CapsuleCD</code> needed to have a Ruby interpreter installed on their build machine, even
if all they were just trying to do was package a Python library. This made my Docker containers bloated,
and more complicated to develop. Wouldn’t it be nice to just have single binary I could download into the
container? And so the migration to Golang began, if only in my head at that point.</p>

<p>Over the next couple months, I kept going back to that idea, and a couple weeks ago, I finally sat down and
started porting my ~3000 line Ruby application to Golang. While I could have just bought a book like Golang
for Dummies, I decided to just jump into the coding, and just read blog posts and stack overflow when I got stuck.</p>

<p>I can already hear some of you cringing. To be honest, while I was having a lot of fun, my initial development
was pretty slow. I was trying to write an application in a new language, without knowing any of the conventions.
The thing is, I loved it. Those “Ah-Ha!” moments and getting things compiling again after a huge refactor
were an incredible motivator.</p>

<p>Here’s a bunch of the unexpected/unconventional things I learned while porting my app to Golang.</p>

<blockquote>
  <p>Please note, these are things that I didn’t <strong>expect</strong> when I started writing Golang code with a
background in popular typed and dynamically typed languages, (C++, C#, Java, Ruby, Python and NodeJS).
These are not necessarily criticisms of Golang. I was able to go from 0 -&gt; working release of my software
in a completely new language in 2 weeks. That’s pretty awesome if you ask me.</p>
</blockquote>

<h1 id="before-your-first-line">Before your first line.</h1>

<h2 id="package-layout">Package Layout</h2>
<p>While not required for a compiled language, I was still unprepared for the fact that there doesn’t seem to be a <strong>Standard™</strong> folder structure for a Golang library, like there is for Ruby, Chef &amp; Node. There seem to be a couple of popular community structures, and I found myself liking <a href="https://peter.bourgon.org/go-best-practices-2016/#repository-structure">Peter Bourgon’s recommendations</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>github.com/peterbourgon/foo/
  circle.yml
  Dockerfile
  cmd/
    foosrv/
      main.go
    foocli/
      main.go
  pkg/
    fs/
      fs.go
      fs_test.go
      mock.go
      mock_test.go
    merge/
      merge.go
      merge_test.go
    api/
      api.go
      api_test.go
</code></pre></div></div>

<h2 id="circular-dependencies">!Circular Dependencies</h2>
<p>Package layout becomes even more important when you find out that Golang does’t support circular
dependencies between packages. If A imports B, and B imports A, Golang will give up and complain.
I actually kinda like it, as it forced me to think a bit more about my application’s domain model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import cycle not allowed
package github.com/AnalogJ/dep/a
  imports github.com/AnalogJ/dep/b
  imports github.com/AnalogJ/dep/a
</code></pre></div></div>

<h2 id="dependency-management">Dependency management</h2>
<p><code class="language-plaintext highlighter-rouge">npm</code>, <code class="language-plaintext highlighter-rouge">pypi</code>, <code class="language-plaintext highlighter-rouge">bundler</code>. Each of these package managers are synonymous with their language. However Golang
doesn’t have an official package manger (<a href="https://github.com/golang/dep">yet</a>). In the meantime the community
has come up with a <a href="https://github.com/Masterminds/glide">couple</a> <a href="https://github.com/FiloSottile/gvt">of</a>
<a href="https://github.com/kardianos/govendor">good</a> <a href="https://github.com/FiloSottile/gvt">alternatives</a>. The problem
is that they are all really good, and it can be a bit daunting to pick one. I ended up choosing <a href="https://github.com/Masterminds/glide">Glide</a>,
because it has a similar feel to <code class="language-plaintext highlighter-rouge">bundler</code> and <code class="language-plaintext highlighter-rouge">npm</code>.</p>

<h2 id="documentation">Documentation</h2>
<p>This is actually one of the best things about Golang. <code class="language-plaintext highlighter-rouge">go docs</code> and the <code class="language-plaintext highlighter-rouge">godoc.org</code> site are awesome
and standardize the documentation for any library you might use. This is a nice step up from the NodeJS
community where all package documentation is custom and self hosted.</p>

<h2 id="goroot-gopath">GOROOT, GOPATH</h2>
<p>Golang imports are done in a kind of weird way. Unlike most other languages, Golang basically requires
that your source live in pre-configured folder(s). I’m not going to delve into the details, but you should
know that it takes a bit of setup &amp; getting used to. Dmitri Shuralyov’s <a href="https://dmitri.shuralyov.com/blog/18">How I use GOPATH with multiple
workspaces</a> is a great resource.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GOPATH=/landing/workspace/path:/personal/workspace/path:/corporate/workspace/path
</code></pre></div></div>

<h1 id="scratching-that-itch">Scratching that Itch.</h1>

<h2 id="pseudo-class-struct-inheritance">Pseudo <del>Class</del> Struct Inheritance</h2>
<p>The Golang developers did some interesting things when designing the inheritance model. Instead of using
one of the more conventional inheritance models of typed languages like multiple-inheritance or classical
inheritance, Golang follows a multiple composition pattern similar to Ruby.
<a href="https://github.com/luciotato/golang-notes/blob/master/OOP.md#method-shadowing">Method-Shadowing</a> can
cause some unexpected results if not understood completely.</p>

<h2 id="duck-typed-interfaces">Duck-Typed Interfaces</h2>
<p>This is another cool unexpected feature of Golang. Interfaces are <a href="https://en.wikipedia.org/wiki/Duck_typing">duck-typed</a>,
something I’ve only seen in dynamically typed languages. This duck-typing works hand-in-hand with <code class="language-plaintext highlighter-rouge">struct</code>
composition.</p>

<h2 id="structs-have-fields-interfaces-dont">Structs have fields, Interfaces don’t</h2>
<p>Unfortunately <code class="language-plaintext highlighter-rouge">structs</code> can’t have the same <em>API</em> as <code class="language-plaintext highlighter-rouge">interfaces</code>, as the latter cannot define fields. This
is not a huge issue, as one can just define a getter and a setter method on the interface, but it was a bit
confusing. I’m sure theres a good technical/CS theory answer for why this is, but yeah.</p>

<h2 id="publicprivate-naming">Public/Private naming</h2>
<p>Golang took Python’s <code class="language-plaintext highlighter-rouge">public</code> and <code class="language-plaintext highlighter-rouge">private</code> method naming scheme one level further. When I initially found
out that functions, methods and struct names starting with an uppercase character are public and lowercase
are private, I wasn’t sure how to feel about it. But honestly, after working with Golang for 2 weeks, I
really like this convention.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type PublicStructName struct {}
type privateStructName struct {}
</code></pre></div></div>

<h2 id="defer">defer</h2>
<p>Another surprisingly useful feature Golang. I’m sure it’s a result of Golang’s parallel processing and
error model, but <code class="language-plaintext highlighter-rouge">defer</code>’s make it really easy to keep your cleanup close to the originating code. Mentally
I treat it like an alternative to a <code class="language-plaintext highlighter-rouge">finally</code> method in the <code class="language-plaintext highlighter-rouge">try-catch-finally</code> pattern or the <code class="language-plaintext highlighter-rouge">using</code>
block in <code class="language-plaintext highlighter-rouge">C#</code>/<code class="language-plaintext highlighter-rouge">Java</code> but I’m sure there are more creative uses for it.</p>

<h2 id="go-fmt-is-awesome"><code class="language-plaintext highlighter-rouge">go fmt</code> is awesome</h2>
<p>You’ll never have the “tabs vs spaces” debate with a Golang developer. There is a standardized Golang
style and <code class="language-plaintext highlighter-rouge">go fmt</code> can reformat your code to comply with it. It’s a neat tool, and reading its source
introduced me to the powerful <a href="https://golang.org/pkg/go/parser/"><code class="language-plaintext highlighter-rouge">parser</code></a> and <a href="https://golang.org/pkg/go/ast/"><code class="language-plaintext highlighter-rouge">ast</code></a> libraries.</p>

<h2 id="goarch-goos-cgo--cross-compiling">GOARCH, GOOS, CGO &amp; Cross Compiling</h2>
<p>My goal of creating a single standalone <code class="language-plaintext highlighter-rouge">CapsuleCD</code> binary is the entire reason I started my port
to Golang. However it quickly became apparent that simple static binaries aren’t an intrinsic feature
of Golang (which should have been obvious). If your code is all written in vanilla Golang, and the code
of all your dependencies (and their dependencies), then you can <a href="http://golangcookbook.com/chapters/running/cross-compiling/">build static binaries</a>
to your heart’s content using <code class="language-plaintext highlighter-rouge">GOOS</code> and <code class="language-plaintext highlighter-rouge">GOARCH</code>. However if you’re unlucky like I was, and you have
a dependency that calls <code class="language-plaintext highlighter-rouge">C</code> code under the hood (by importing a <code class="language-plaintext highlighter-rouge">C pseudo-package</code>) then you’re in for
a world of pain. Don’t get me wrong, creating a dynamically linked binary is still super easy. But to
generate a static binary, with no external dependencies, means you need to ensure that all your
<code class="language-plaintext highlighter-rouge">C</code> dependencies (and their dependencies) are all statically linked too. Like I said, obvious.
<code class="language-plaintext highlighter-rouge">C pseudo-packages</code> are compiled via <code class="language-plaintext highlighter-rouge">CGO</code>, and you’ll need to look at the documentation to find all
the compiler flags necessary to help <code class="language-plaintext highlighter-rouge">CGO</code> locate your static libraries. A table of all supported GOOS
and GOARCH pairs is located in the <a href="https://golang.org/doc/install/source#environment">Golang docs</a></p>

<h1 id="how-do-i-test-this">How do I test this?</h1>

<h2 id="hidden-in-plain-sight">Hidden in plain sight</h2>
<p>Test files are suffixed with <code class="language-plaintext highlighter-rouge">_test.go</code> and should be located side-by-side with the code they test,
rather than relegated to a special testing folder. Its nice, even though it feels a bit cluttered at first.</p>

<p>Test data goes in a special <code class="language-plaintext highlighter-rouge">testdata</code> folder. Both the <code class="language-plaintext highlighter-rouge">testdata</code> folder and <code class="language-plaintext highlighter-rouge">_test.go</code> files are completely
ignored by the compiler during <code class="language-plaintext highlighter-rouge">go build</code>.</p>

<h2 id="go-list-and-vendor-folder"><code class="language-plaintext highlighter-rouge">go list</code> and <code class="language-plaintext highlighter-rouge">vendor</code> folder</h2>
<p>So, dependency management is pretty new to the Golang language, and not all tools understand the special
<code class="language-plaintext highlighter-rouge">vendor</code> folder. As such, when you run <code class="language-plaintext highlighter-rouge">go test</code>, by default you’ll find it running the tests of all your
dependencies. Use <code class="language-plaintext highlighter-rouge">go list | grep -v /vendor</code> to get Golang to ignore the vendor folder.</p>

<p><code class="language-plaintext highlighter-rouge">go fmt $(go list ./... | grep -v /vendor/)</code></p>

<h2 id="if-err--nil"><code class="language-plaintext highlighter-rouge">if err != nil</code></h2>
<p>I’m a stickler for code coverage. I try to keep my open source projects above 80% coverage, but I’m having
a hard time doing that with Golang. Those of you already familiar with Golang will probably just point out that
Golang is one of the easiest languages to get <a href="https://blog.golang.org/cover">good coverage in</a>. Rather
than creating a seperate execution path for errors (<code class="language-plaintext highlighter-rouge">try-catch-finally</code>) Golang treats all errors as standard objects.
Golang convention states that functions which can produce errors should return them as it’s last <code class="language-plaintext highlighter-rouge">return</code> argument.</p>

<p>It’s a pretty interesting model, which reminds me a bit of <code class="language-plaintext highlighter-rouge">Node</code>’s built-in functions. However, just like <code class="language-plaintext highlighter-rouge">Node</code>, it can
be difficult to write unit tests that produce errors in built-in functionality. This becomes even more annoying when you follow
a coding pattern where you bubble-up errors, and then handle them at a higher level. When doing this, you’ll write alot of
code the looks like the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data, err := myfunction(...)
if(err != nil){
	return err
}

data2, err2 := myfunction2(...)
if(err2 != nil){
	return err
}

</code></pre></div></div>

<p>This starts to clutter up your code pretty quick. At this point some of you may be thinking that <code class="language-plaintext highlighter-rouge">interface</code>s and <code class="language-plaintext highlighter-rouge">mock</code>s
would solve these problems. While that’s true in some cases, I don’t think it makes sense to write massive <code class="language-plaintext highlighter-rouge">interface</code>s for
built-in libraries like <code class="language-plaintext highlighter-rouge">os</code> and <code class="language-plaintext highlighter-rouge">ioutil</code>, or pass those libraries in as arguments, just so that we can artifically generate
errors for <code class="language-plaintext highlighter-rouge">ioutil.WriteFile</code> and <code class="language-plaintext highlighter-rouge">os.MkdirAll</code>.</p>

<p>I’m that this is definitely a shortcoming in my mental-model, but I’ve read a ton of documentation and blog posts on how
unit tests and code coverage should be done in Golang, and I still haven’t found a pattern that makes sense without
seeming to require a dependency injection engine of some sort, something that Golang seems to actively dislike as too cumbersome.</p>

<h1 id="conclusion">Conclusion</h1>

<p>I’d love to hear your thoughts. I’ve only been working with Golang for a few weeks, but it’s been an incredibly educational
and enjoyable experience. I was able to go from no experience to building a real, working application in Golang in very
little time, not just toy examples from some book. I know that I’m no expert in Golang yet, and that there are still  theory gaps
in my understanding of Golang, but I feel like they are much further apart than I expected when I went down this <code class="language-plaintext highlighter-rouge">self-taught without books</code> path.</p>

<p>Golang worked exactly as I thought it would, giving me <a href="https://github.com/AnalogJ/capsulecd/releases">binaries</a> that I can easily download
onto slim Docker containers, without requiring a Ruby interpreter. If you maintain executables in other languages, I would
definitely recommend you consider giving Golang a try.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Local Development with Wildcard DNS</title>
	  <link>/local-development-with-wildcard-dns</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2017-04-07T04:19:33-05:00</pubDate>
	  <guid>/local-development-with-wildcard-dns</guid>
	  <description><![CDATA[
	     <p>The holy-grail of local development is wildcard DNS: the ability to have <code class="language-plaintext highlighter-rouge">*.local.company.com</code> pointing to <code class="language-plaintext highlighter-rouge">localhost</code>, your development machine.
It doesn’t matter if you’re working on <code class="language-plaintext highlighter-rouge">website.local.company.com</code> or <code class="language-plaintext highlighter-rouge">api.local.company.com</code>, there’s no additional configuration necessary as you start working on new projects.</p>

<p>Unfortunately macOS doesn’t support wildcard entries in the <code class="language-plaintext highlighter-rouge">/etc/hosts</code> file – no OS does out of the box.</p>

<h2 id="dnsmasq">Dnsmasq</h2>

<p><a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">Dnsmasq</a> is a tiny and incredibly popular DNS server that you can run locally, and supports wildcard domain resolution with very little configuration.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>dnsmasq
</code></pre></div></div>

<p>Now lets setup the configuration directory and configure <code class="language-plaintext highlighter-rouge">dnsmasq</code> to resolve all of our development domains.</p>

<blockquote>
  <p>You’ll want to avoid the <code class="language-plaintext highlighter-rouge">*.dev</code> and <code class="language-plaintext highlighter-rouge">*.local</code> domains for development. <code class="language-plaintext highlighter-rouge">.dev</code> exists as a real <a href="https://newgtlds.icann.org/en/program-status/delegated-strings">TLD in the ICANN root</a>. <code class="language-plaintext highlighter-rouge">.local</code> is used by the <a href="https://support.apple.com/en-us/HT201275">Bonjour service</a> on macOS. I recommend using <code class="language-plaintext highlighter-rouge">*.local.companyname.com</code> or <code class="language-plaintext highlighter-rouge">*.lan</code></p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-pv</span> <span class="si">$(</span>brew <span class="nt">--prefix</span><span class="si">)</span>/etc/

<span class="nb">cat</span> <span class="o">&gt;</span><span class="si">$(</span>brew <span class="nt">--prefix</span><span class="si">)</span>/etc/dnsmasq.conf <span class="o">&lt;&lt;</span><span class="no">EOL</span><span class="sh">

# Add domains which you want to force to an IP address here.
# The example below send any host in *.local.company.com and *.lan to a local
# webserver.
address=/local.company.com/127.0.0.1
address=/lan/127.0.0.1

# Don't read /etc/resolv.conf or any other configuration files.
no-resolv
# Never forward plain names (without a dot or domain part)
domain-needed
# Never forward addresses in the non-routed address spaces.
bogus-priv
</span><span class="no">
EOL
</span></code></pre></div></div>

<p>Then lets configure <code class="language-plaintext highlighter-rouge">launchd</code> start <code class="language-plaintext highlighter-rouge">dnsmasq</code> now and restart at startup:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>brew services start dnsmasq
</code></pre></div></div>

<p>Finally lets validate that our <code class="language-plaintext highlighter-rouge">dnsmasq</code> server is configured to respond to all subdomains of <code class="language-plaintext highlighter-rouge">local.company.com</code> by running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>dig nested.test.local.company.com @127.0.0.1

<span class="p">;</span> &lt;&lt;<span class="o">&gt;&gt;</span> DiG 9.8.3-P1 &lt;&lt;<span class="o">&gt;&gt;</span> nested.test.local.company.com @127.0.0.1
<span class="p">;;</span> global options: +cmd
<span class="p">;;</span> Got answer:
<span class="p">;;</span> -&gt;&gt;HEADER<span class="o">&lt;&lt;-</span> <span class="no">opcode</span><span class="sh">: QUERY, status: NOERROR, id: 64864
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;nested.test.local.company.com.	IN	A

;; ANSWER SECTION:
nested.test.local.company.com. 0 IN	A	127.0.0.1

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Sat Apr  8 11:15:17 2017
;; MSG SIZE  rcvd: 63
</span></code></pre></div></div>

<h2 id="integration-using-etcresolver">Integration using <code class="language-plaintext highlighter-rouge">/etc/resolver</code></h2>

<p>At this point we have a working DNS server, but it’s meaningless because macOS won’t use it for resolving any domains.</p>

<p>We can change this by adding configuration files in the <code class="language-plaintext highlighter-rouge">/etc/resolver</code> directory.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>bash <span class="nt">-c</span> <span class="s1">'echo "nameserver 127.0.0.1" &gt; /etc/resolver/local.company.com'</span>
<span class="nb">sudo </span>bash <span class="nt">-c</span> <span class="s1">'echo "nameserver 127.0.0.1" &gt; /etc/resolver/lan'</span>
</code></pre></div></div>

<p>Each domain that we configured in <code class="language-plaintext highlighter-rouge">dnsmasq</code> should have a corresponding entry in <code class="language-plaintext highlighter-rouge">/etc/resolver/</code></p>

<p>Next, lets test that our resolver entries have been picked up by macOS.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>scutil <span class="nt">--dns</span>

...
resolver <span class="c">#8</span>
  domain   : local.company.com
  nameserver[0] : 127.0.0.1
  flags    : Request A records
Reachable, Directly Reachable Address
...
</code></pre></div></div>

<h2 id="fin">Fin</h2>

<p>Testing you new configuration is easy; just use ping check that you can now resolve your local subdomains:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Make sure you haven't broken your DNS.</span>
ping <span class="nt">-c</span> 1 www.google.com

<span class="c"># Check that .local.company.com &amp; .lan names work</span>
ping <span class="nt">-c</span> 1 this.is.a.test.local.company.com
ping <span class="nt">-c</span> 1 this.domain.does.not.exist.lan
</code></pre></div></div>

<p>This is useful in particular for developers of microservices: your orchestration platform can dynamically generate hostnames, and you won’t have to worry about your <code class="language-plaintext highlighter-rouge">/etc/hosts</code> file again.</p>

<h3 id="references">References</h3>
<ul>
  <li>http://asciithoughts.com/posts/2014/02/23/setting-up-a-wildcard-dns-domain-on-mac-os-x/</li>
  <li>https://gist.github.com/eloypnd/5efc3b590e7c738630fdcf0c10b68072</li>
  <li>https://passingcuriosity.com/2013/dnsmasq-dev-osx/</li>
  <li>http://serverfault.com/questions/118378/in-my-etc-hosts-file-on-linux-osx-how-do-i-do-a-wildcard-subdomain</li>
  <li>https://gist.github.com/ogrrd/5831371</li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>Automating SSL Certificates using Nginx & Letsencrypt - Without the Catch 22</title>
	  <link>/automating-ssl-certificates-using-nginx</link>
	  <author>Jason Kulatunga</author>
	  <pubDate>2016-01-31T23:49:09-06:00</pubDate>
	  <guid>/automating-ssl-certificates-using-nginx</guid>
	  <description><![CDATA[
	     <p>There’s a ton of smart people out there who’ve written guides on <a href="https://sysops.forlaravel.com/letsencrypt">setting</a> <a href="https://blog.rudeotter.com/lets-encrypt-ssl-certificate-nginx-ubuntu/">up</a> <a href="https://davidzych.com/setting-up-ssl-with-lets-encrypt-on-ubuntu-and-nginx/">Nginx</a>, <a href="https://community.letsencrypt.org/t/howto-easy-cert-generation-and-renewal-with-nginx/3491/2">and</a> <a href="https://adambard.com/blog/using-letsencrypt-with-nginx/">automating</a> <a href="https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04">Letsencrypt</a> —but none that setup automation and work 100% correctly out of the box. That’s the goal here, I’ll be documenting all the steps required to get your web application protected by an automatically renewing SSL certificate.</p>

<blockquote>
  <p>NOTE: The following commands will require root user permissions.
You might want to run <code class="language-plaintext highlighter-rouge">sudo su -</code> first.</p>
</blockquote>

<h2 id="installing-nginx">Installing Nginx</h2>

<p>The first step is to install Nginx. If all you want to do is install the standard version, it should be available via your distro’s package manager.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># install Nginx on Ubuntu</span>
apt-get update
apt-get <span class="nb">install</span> <span class="nt">-y</span> nginx
</code></pre></div></div>

<h2 id="install-letsencryptsh">Install Letsencrypt.sh</h2>

<p>The second step is to install a Letsencrypt client. The <a href="https://github.com/letsencrypt/letsencrypt">official client</a> is a bit bloated and complicated to setup. I prefer to use the <a href="https://github.com/lukas2511/letsencrypt.sh">letsencrypt.sh client</a> instead as its code is easier to understand, has few dependencies and its incredibly simple to automate.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># install letsencrypt.sh dependencies (most should already be installed)</span>
apt-get <span class="nb">install</span> <span class="nt">-y</span> openssl curl <span class="nb">sed grep mktemp </span>git

<span class="c"># install letsencrypt.sh into /srv/letsencrypt</span>
git clone https://github.com/lukas2511/letsencrypt.sh.git /srv/letsencrypt
</code></pre></div></div>

<h2 id="configure-letsencrypt">Configure Letsencrypt</h2>

<p>Letsencrypt.sh requires some configuration, but not much, the defaults work out of the box. That means that all you need to do is</p>

<ul>
  <li>create a domains.txt file with the url(s) of the site(s) you’re generating ssl certificates for</li>
  <li>create a acme-challenges folder that can be  accessed by Nginx.</li>
</ul>

<p>Here’s how we can do that.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># First we need to make the client executable</span>
<span class="nb">chmod</span> +x /srv/letsencrypt/letsencrypt.sh
<span class="c"># Then we need to create an ACME challenges folder and symlink it for Nginx to use</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> /srv/letsencrypt/.acme-challenges
<span class="nb">mkdir</span> <span class="nt">-p</span> /var/www/
<span class="nb">ln</span> <span class="nt">-s</span> /srv/letsencrypt/.acme-challenges /var/www/letsencrypt
</code></pre></div></div>

<p>Finally we need to specify the site(s) that will be protected by Letsencrypt ssl certificates.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo "www.example.com" &gt;&gt; /srv/letsencrypt/domains.txt
</code></pre></div></div>

<p>Read more about the domains.txt file format <a href="https://github.com/lukas2511/letsencrypt.sh#domainstxt">here</a></p>

<h2 id="configure-nginx-without-the-catch-22">Configure Nginx (without the Catch-22)</h2>

<p>Up to now, the steps I’ve shown have been the same as almost any other Letsencrypt+Nginx guide you’ve seen online. However most of other guides will tell you to configure Nginx in a way that requires manual intervention.</p>

<p>A basic Letsencrypt Nginx configuration file looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># DONT USE THIS, IT WONT WORK.

# /etc/nginx/sites-enabled/example.conf
# HTTP server
server {
	listen      80;
	server_name www.example.com;
	location '/.well-known/acme-challenge' {
		default_type "text/plain";
		alias /var/www/letsencrypt;
	}
	location / {
		return 301 https://$server_name$request_uri;
	}
}
# HTTPS
server {
	listen       443;
	server_name  www.example.com;
	ssl                  on;
	ssl_certificate      /srv/letsencrypt/certs/www.example.com/fullchain.pem;
	ssl_certificate_key  /srv/letsencrypt/certs/www.example.com/privkey.pem;

	...
}
</code></pre></div></div>

<p>There’s a problem with this though. If you try starting up your Nginx server with this config, it’ll throw an error because the SSL certificate files don’t exist. And you can’t start the letencrypt.sh command to generate the SSL certificates without a working Nginx server to serve up the acme-challenge folder. Classic catch 22.</p>

<p>Here’s the solution: we’re going to break up the Nginx configuration into 2 separate configuration files, one for the  HTTP endpoint with letsencrypt challenge files and one for the HTTPS endpoint serving the actual web application.</p>

<p>We’ll then place them both in the <code class="language-plaintext highlighter-rouge">sites-available</code> folder rather than the standard <code class="language-plaintext highlighter-rouge">sites-enabled</code> folder. By default, any configuration files in the <code class="language-plaintext highlighter-rouge">sites-enabled</code> folder are automatically parsed by Nginx when it’s restarted, however we want to control this process.</p>

<p>The HTTP Nginx configuration file will be located at: <code class="language-plaintext highlighter-rouge">/etc/nginx/sites-available/http.example.conf</code> and look like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># HTTP server
server {
	listen      80;
	server_name www.example.com;
	location '/.well-known/acme-challenge' {
		default_type "text/plain";
		alias /var/www/letsencrypt;
	}
	location / {
		return 301 https://$server_name$request_uri;
	}
}
</code></pre></div></div>

<p>The HTTPS Nginx configuration file will be located at <code class="language-plaintext highlighter-rouge">/etc/nginx/sites-available/https.example.conf</code> and look like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># HTTPS
server {
	listen       443;
	server_name  www.example.com;
	ssl                  on;
	ssl_certificate      /srv/letsencrypt/certs/www.example.com/fullchain.pem;
	ssl_certificate_key  /srv/letsencrypt/certs/www.example.com/privkey.pem;

	#Include actual web application configuration here.
}
</code></pre></div></div>

<h2 id="controlling-nginx">Controlling Nginx</h2>

<p>Before we do anything else, we’ll need to first stop the running Nginx service.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>service nginx stop
</code></pre></div></div>

<p>Then we need to enable the HTTP endpoint by creating a symlink from the <code class="language-plaintext highlighter-rouge">sites-available</code> file to the <code class="language-plaintext highlighter-rouge">sites-enabled</code> folder, and starting the Nginx service</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Enable the http endpoint"</span>
<span class="nb">ln</span> <span class="nt">-s</span> /etc/nginx/sites-available/http.example.conf /etc/nginx/sites-enabled/http.example.conf

<span class="nb">echo</span> <span class="s2">"Starting nginx service..."</span>
service nginx start
</code></pre></div></div>

<p>At this point we have a working HTTP endpoint which will correctly serve up any files in the <code class="language-plaintext highlighter-rouge">acme-challenge</code> folder. Lets generate some certificates.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Generate Letsencrypt SSL certificates"</span>
/srv/letsencrypt/letsencrypt.sh <span class="nt">--cron</span>
</code></pre></div></div>

<p>After the certificates are generated successfully by Letsencrypt.sh, we’ll have to enable our HTTPS endpoint, which is where all standard traffic is being redirected to.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Enable the https endpoint"</span>
<span class="nb">ln</span> <span class="nt">-s</span> /etc/nginx/sites-available/https.example.conf /etc/nginx/sites-enabled/https.example.conf
</code></pre></div></div>

<p>Finally, we need to tell Nginx update its configuration, as we’ve just added the HTTPS endpoint, but we want to do it without any downtime. Thankfully the Nginx developers have provided us a way to do that.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Reload nginx service..."</span>
service nginx reload
</code></pre></div></div>

<p>Now we have a working HTTPS enabled web application. The only thing left to do is automate the certificate renewal.</p>

<h2 id="downtime-free-automatic-certificate-renewal">Downtime-Free Automatic Certificate Renewal</h2>

<p>Automatically renewing your SSL certificate isn’t just a cool feature of Letsencrypt.sh, its actually almost a requirement. By default Letsencrypt certificates expire every 90 days, so renewing it manually would pretty annoying. Thankfully it only takes a single command to completely automate this process.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Register Letsencrypt to run weekly"</span>
<span class="nb">echo</span> <span class="s2">"5 8 * * 7 root /srv/letsencrypt/letsencrypt.sh --cron &amp;&amp; service nginx reload"</span> <span class="o">&gt;</span> /etc/cron.d/letsencrypt.sh
<span class="nb">chmod </span>u+x  /etc/cron.d/letsencrypt.sh
</code></pre></div></div>

<p>That command will register a new cron task to run every week that will run the letsencrypt.sh command. If the letsencrypt.sh script detects that the certificate will expire within 30 days, the certificates will be renewed automatically, and the Nginx server will reload, without any downtime.</p>

<h2 id="fin">Fin</h2>

<p>At this point you should have a working SSL protected web application, with automatic certificate renewal, at the cost of a handful of bash commands.</p>

<p>If you’re looking for an example of how this process can be used to automatically protect a website running inside a Docker container, look no further than my minimal <a href="https://github.com/AnalogJ/letsencrypt-http01-docker-nginx-example">letsencrypt-http01-docker-nginx-example</a> repo.</p>

<div class="github-widget" data-repo="AnalogJ/letsencrypt-http01-docker-nginx-example"></div>

<p>If you would like to see a more real world use of Letsencrypt with Nginx and automation you should check my <a href="https://github.com/AnalogJ/gitmask">Gitmask</a> repo.</p>

	  ]]></description>
	</item>


</channel>
</rss>
